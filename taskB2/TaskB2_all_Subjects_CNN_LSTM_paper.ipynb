{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as cp\n",
    "from sliding_window import sliding_window\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Conv2D, LSTM, CuDNNLSTM, Flatten, Dropout, Input, TimeDistributed, Reshape\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sensors = 113\n",
    "win_size = 24\n",
    "classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      " ..from file oppChallenge_gestures.data\n",
      " ..reading instances: train (557963, 113), test (118750, 113)\n"
     ]
    }
   ],
   "source": [
    "def  load_dataset(filename):\n",
    "\n",
    "    f = open(filename, 'rb')\n",
    "    data = cp.load(f)\n",
    "    f.close()\n",
    "\n",
    "    X_train, y_train = data[0]\n",
    "    X_test, y_test = data[1]\n",
    "\n",
    "    print(\" ..from file {}\".format(filename))\n",
    "    print(\" ..reading instances: train {0}, test {1}\".format(X_train.shape, X_test.shape))\n",
    "\n",
    "    X_train = X_train.astype(np.float32)\n",
    "    X_test = X_test.astype(np.float32)\n",
    "\n",
    "    # The targets are casted to int8 for GPU compatibility.\n",
    "    y_train = y_train.astype(np.uint8)\n",
    "    y_test = y_test.astype(np.uint8)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "print(\"Loading data...\")\n",
    "X_train, y_train, X_test, y_test = load_dataset('oppChallenge_gestures.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.50666666,  0.67183334,  0.50816667, ...,  0.49810001,\n",
       "         0.50105   ,  0.83600003],\n",
       "       [ 0.50816667,  0.67383331,  0.50700003, ...,  0.49654999,\n",
       "         0.50059998,  0.83600003],\n",
       "       [ 0.50283331,  0.67483336,  0.514     , ...,  0.4975    ,\n",
       "         0.50050002,  0.83600003],\n",
       "       ..., \n",
       "       [ 0.5       ,  0.5       ,  0.5       , ...,  0.49880001,\n",
       "         0.50064999,  0.53200001],\n",
       "       [ 0.5       ,  0.5       ,  0.5       , ...,  0.49994999,\n",
       "         0.50160003,  0.53600001],\n",
       "       [ 0.5       ,  0.5       ,  0.5       , ...,  0.5       ,\n",
       "         0.5       ,  0.5       ]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'assert num_sensors == X_train.shape[1]\\ndef opp_sliding_window(data_x, data_y, ws, ss):\\n    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\\n    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\\n    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\\n\\n# Sensor data is segmented using a sliding window mechanism\\nX_test, y_test = opp_sliding_window(X_test, y_test, win_size, win_size/2)\\nprint(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\\n\\n# Data is reshaped since the input of the network is a 4 dimension tensor\\n#X_test = X_test.reshape((-1, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"assert num_sensors == X_train.shape[1]\n",
    "def opp_sliding_window(data_x, data_y, ws, ss):\n",
    "    data_x = sliding_window(data_x,(ws,data_x.shape[1]),(ss,1))\n",
    "    data_y = np.asarray([[i[-1]] for i in sliding_window(data_y,ws,ss)])\n",
    "    return data_x.astype(np.float32), data_y.reshape(len(data_y)).astype(np.uint8)\n",
    "\n",
    "# Sensor data is segmented using a sliding window mechanism\n",
    "X_test, y_test = opp_sliding_window(X_test, y_test, win_size, win_size/2)\n",
    "print(\" ..after sliding window (testing): inputs {0}, targets {1}\".format(X_test.shape, y_test.shape))\n",
    "\n",
    "# Data is reshaped since the input of the network is a 4 dimension tensor\n",
    "#X_test = X_test.reshape((-1, 1, SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class trainerHelper():\n",
    "    \n",
    "    def __init__(self, data, labels, win_size, num_sensor_data, num_classes):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.win_size = win_size\n",
    "        self.sensor_data = num_sensor_data\n",
    "        self.num_classes = num_classes\n",
    "        self.data_length = self.data.shape[0]\n",
    "        print(self.data_length)\n",
    "        self.start = 0\n",
    "        \n",
    "    def windows(self):\n",
    " \n",
    "        while self.start + self.win_size < self.data_length:\n",
    "            yield int(self.start), int(self.start + self.win_size)\n",
    "            self.start += (self.win_size/2)\n",
    "    \n",
    "    def segment_data(self):\n",
    "        self.start = 0\n",
    "        segments = np.empty((0, self.win_size, self.sensor_data))\n",
    "        labels= np.empty((0, self.num_classes))\n",
    "        for (start, end) in self.windows():\n",
    "            x = np.zeros((1, self.win_size, self.sensor_data))\n",
    "            x[0,:] = self.data[start:end,:]\n",
    "            label = self.labels[start:end]\n",
    "            if(x.shape[1] == self.win_size):\n",
    "                segments = np.vstack([segments,x])\n",
    "                lb = np.zeros((1, self.num_classes))\n",
    "                lb[0,:] = label[-1]\n",
    "                labels = np.vstack([labels,lb])\n",
    "        return segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557963,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "557963\n"
     ]
    }
   ],
   "source": [
    "trainer_helper = trainerHelper(X_train, y_train, win_size, num_sensors, classes)\n",
    "segments, labels = trainer_helper.segment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "size_of_kernel = (5,1)\n",
    "kernel_strides = 1\n",
    "num_filters = 64\n",
    "num_lstm_cells = 128\n",
    "dropout_prob = 0.5\n",
    "inputshape = (win_size, num_sensors, 1)\n",
    "batchshape = (None, win_size, num_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 activation='relu', input_shape=inputshape, name='1_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 activation='relu', name='2_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 activation='relu', name='3_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 activation='relu', name='4_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Reshape((8, num_filters*num_sensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(CuDNNLSTM(num_lstm_cells, return_sequences=True, name='1_lstm_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='2_dropout_layer'))\n",
    "\n",
    "model.add(CuDNNLSTM(num_lstm_cells, return_sequences=False, name='2_lstm_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='3_dropout_layer'))\n",
    "\n",
    "model.add(Dense(classes, activation='softmax', name='softmax_layer'))\n",
    "\n",
    "rms = optimizers.RMSprop(lr=0.001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(str(layer.name) + ': input shape: ' + str(layer.input_shape) + ' output shape: ' + str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "train_epoches = 50\n",
    "model.fit(reshaped_train,train_segments_labels,validation_data=(reshaped_val,val_segments_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(reshaped_test,test_segments_labels,verbose=1)\n",
    "print(score)\n",
    "model.save('taskB2_all_Subjects_CNN_LSTM_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(reshaped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0.0\n",
    "den = 0.0\n",
    "for pair in zip(predictions, test_segments_labels):\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    den += 1.0\n",
    "\n",
    "print(count / den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score measure\n",
    "from sklearn.metrics import f1_score\n",
    "num_classes = 18\n",
    "class_predictions = []\n",
    "class_true = []\n",
    "tot_labels = 0.0\n",
    "count = 0.0\n",
    "for pair in zip(predictions, test_segments_labels):\n",
    "    class_predictions.append(np.argmax(pair[0]))\n",
    "    class_true.append(np.argmax(pair[1]))\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    tot_labels += 1.0\n",
    "    \n",
    "print('Standard accuracy is ' + str(count/tot_labels))    \n",
    "\n",
    "unique, counts = np.unique(class_true, return_counts=True)\n",
    "counted_labels = dict(zip(unique, counts))\n",
    "f1_scores = f1_score(class_predictions, class_true, average=None)\n",
    "\n",
    "tot_f1_score = 0.0\n",
    "weights_sum = 0.0\n",
    "for i in range(num_classes):\n",
    "    labels_class_i = counted_labels[i]\n",
    "    weight_i = labels_class_i / tot_labels\n",
    "    weights_sum += weight_i\n",
    "    tot_f1_score += f1_scores[i]*weight_i\n",
    "    print(str(i) + ' ' + str(weight_i) + ' ' + str(f1_scores[i]))\n",
    "\n",
    "    \n",
    "print('The weigths sum is ' + str(weights_sum))\n",
    "print('The computed f1-score is {}'.format(tot_f1_score))\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(class_true, class_predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.to_csv('preds_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(testY)\n",
    "true_df.to_csv('true_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
