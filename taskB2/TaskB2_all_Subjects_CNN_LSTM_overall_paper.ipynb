{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, LSTM, CuDNNLSTM, Flatten, Dropout, Reshape, BatchNormalization, PReLU, ELU, LeakyReLU\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindow(sequence, labels, winSize, step, noNull):\n",
    "\n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # number of chunks\n",
    "    numOfChunks = ((len(sequence)-winSize)//step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        segment = sequence[i:i+winSize]\n",
    "        seg_labels = labels[i:i+winSize]\n",
    "        if noNull:\n",
    "            if seg_labels[-1] != 0:\n",
    "                yield segment, seg_labels\n",
    "        else:\n",
    "            yield segment, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_frequent(labels):\n",
    "    \n",
    "    (values, counts) = np.unique(labels, return_counts=True)\n",
    "    index = np.argmax(counts)\n",
    "    return values[index]\n",
    "    \n",
    "\n",
    "def segment_data(X_train, y_train, X_val, y_val, X_test, y_test, winSize, step, noNull=False):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    assert len(X_test) == len(y_test)\n",
    "    # obtain chunks of data\n",
    "    train_chunks = slidingWindow(X_train, y_train , winSize, step, noNull)\n",
    "    val_chunks = slidingWindow(X_val, y_val, winSize, step, noNull)\n",
    "    test_chunks = slidingWindow(X_test, y_test, winSize, step, noNull)\n",
    "    \n",
    "    # segment the data\n",
    "    train_segments = []\n",
    "    train_labels = []\n",
    "    for chunk in train_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        train_segments.append(data)\n",
    "        train_labels.append(get_most_frequent(labels))\n",
    "        \n",
    "    val_segments = []\n",
    "    val_labels = []\n",
    "    for chunk in val_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        val_segments.append(data)\n",
    "        val_labels.append(get_most_frequent(labels))\n",
    "    \n",
    "    test_segments = []\n",
    "    test_labels = []\n",
    "    for chunk in test_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        test_segments.append(data)\n",
    "        test_labels.append(get_most_frequent(labels))\n",
    "        \n",
    "    return np.array(train_segments), np.array(train_labels), np.array(val_segments), np.array(val_labels), np.array(test_segments), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, test_data):\n",
    "\n",
    "    train_labels = np.array(train_data['labels'].values)\n",
    "    val_labels = np.array(val_data['labels'].values)\n",
    "    test_labels = np.array(test_data['labels'].values)\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data.drop(['labels'], axis=1, inplace=True)\n",
    "    val_data.drop(['labels'], axis=1, inplace=True)\n",
    "    test_data.drop(['labels'], axis=1, inplace=True)\n",
    "    data = pd.concat([train_data,val_data,test_data])\n",
    "    scaler.fit(data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import train data\n",
    "adl_1_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL1Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL2Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL3Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_4 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL4Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_5 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL5Opportunity_taskB2_S1.csv\",header=None)\n",
    "drill_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/Drill1Opportunity_taskB2.csv\",header=None)\n",
    "adl_2_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL1Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL2Opportunity_taskB2_S2.csv\",header=None)\n",
    "drill_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/Drill2Opportunity_taskB2.csv\",header=None)\n",
    "adl_3_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL1Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL2Opportunity_taskB2_S3.csv\",header=None)\n",
    "drill_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/Drill3Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "# import validation data\n",
    "adl_2_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL3Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL3Opportunity_taskB2_S3.csv\",header=None)\n",
    "\n",
    "# import test data\n",
    "adl_2_4 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL4Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_5 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL5Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_4 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL4Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_5 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL5Opportunity_taskB2_S3.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frames = [adl_1_1,adl_1_2,adl_1_3,adl_1_4,adl_1_5,drill_1,adl_2_1,adl_2_2,drill_2,adl_3_1,adl_3_2,drill_3]\n",
    "val_frames = [adl_2_3,adl_3_3]\n",
    "test_frames = [adl_2_4,adl_2_5,adl_3_4,adl_3_5]\n",
    "train_data = pd.concat(train_frames)\n",
    "val_data = pd.concat(val_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "train_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "val_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "test_data.rename(columns ={113: 'labels'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train (497014, 114), val (60949, 114), test (118750, 114)\n"
     ]
    }
   ],
   "source": [
    "print(\"shapes: train {0}, val {1}, test {2}\".format(train_data.shape, val_data.shape, test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_train, scaled_val, scaled_test, train_labels, val_labels, test_labels = prepare_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sensors = 113\n",
    "window_size = 64\n",
    "step_size = 10\n",
    "classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segments, train_labels, val_segments, val_labels, test_segments, test_labels = segment_data(scaled_train, train_labels, scaled_val, val_labels,\n",
    "                                                                                                  scaled_test, test_labels, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49696,)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "train_labels = encoder.fit_transform(train_labels.reshape(-1,1)).toarray()\n",
    "val_labels = encoder.transform(val_labels.reshape(-1,1)).toarray()\n",
    "test_labels = encoder.transform(test_labels.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input for CNN\n",
    "reshaped_train = train_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_val = val_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_test = test_segments.reshape(-1, window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "kernel_strides = 1\n",
    "dropout_prob = 0.5\n",
    "inputshape = (window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(BatchNormalization(input_shape=inputshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(50, kernel_size=(11,1), strides=kernel_strides,\n",
    "                 kernel_initializer='glorot_normal', name='1_conv_layer'))\n",
    "model.add(ELU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Reshape((27, 50*num_sensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.add(CuDNNLSTM(27,kernel_initializer='glorot_normal', return_sequences=True, name='1_lstm_layer'))\n",
    "\n",
    "model.add(CuDNNLSTM(27,kernel_initializer='glorot_normal',return_sequences=False, name='2_lstm_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='2_dropout_layer'))\n",
    "\n",
    "model.add(Dense(512, kernel_initializer='glorot_normal',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense1_layer'))\n",
    "model.add(ELU())\n",
    "model.add(Dropout(dropout_prob, name='3_dropout_layer'))\n",
    "\n",
    "model.add(Dense(classes,kernel_initializer='glorot_normal',\n",
    "                bias_initializer=initializers.Constant(value=0.1),activation='softmax', name='softmax_layer'))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_2: input shape: (None, 64, 113, 1) output shape: (None, 64, 113, 1)\n",
      "1_conv_layer: input shape: (None, 64, 113, 1) output shape: (None, 54, 113, 50)\n",
      "elu_2: input shape: (None, 54, 113, 50) output shape: (None, 54, 113, 50)\n",
      "max_pooling2d_2: input shape: (None, 54, 113, 50) output shape: (None, 27, 113, 50)\n",
      "reshape_1: input shape: (None, 27, 113, 50) output shape: (None, 27, 5650)\n",
      "1_lstm_layer: input shape: (None, 27, 5650) output shape: (None, 27, 27)\n",
      "2_lstm_layer: input shape: (None, 27, 27) output shape: (None, 27)\n",
      "2_dropout_layer: input shape: (None, 27) output shape: (None, 27)\n",
      "dense1_layer: input shape: (None, 27) output shape: (None, 512)\n",
      "elu_3: input shape: (None, 512) output shape: (None, 512)\n",
      "3_dropout_layer: input shape: (None, 512) output shape: (None, 512)\n",
      "softmax_layer: input shape: (None, 512) output shape: (None, 18)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(str(layer.name) + ': input shape: ' + str(layer.input_shape) + ' output shape: ' + str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49696 samples, validate on 6089 samples\n",
      "Epoch 1/50\n",
      "49696/49696 [==============================] - 49s 994us/step - loss: 1.2194 - acc: 0.6917 - val_loss: 0.7439 - val_acc: 0.8343\n",
      "Epoch 2/50\n",
      "49696/49696 [==============================] - 44s 882us/step - loss: 0.7610 - acc: 0.7592 - val_loss: 0.5249 - val_acc: 0.8394\n",
      "Epoch 3/50\n",
      "49696/49696 [==============================] - 44s 879us/step - loss: 0.5995 - acc: 0.7947 - val_loss: 0.5139 - val_acc: 0.8594\n",
      "Epoch 4/50\n",
      "49696/49696 [==============================] - 43s 873us/step - loss: 0.5246 - acc: 0.8145 - val_loss: 0.4754 - val_acc: 0.8602\n",
      "Epoch 5/50\n",
      "49696/49696 [==============================] - 43s 871us/step - loss: 0.4812 - acc: 0.8269 - val_loss: 0.4998 - val_acc: 0.8568\n",
      "Epoch 6/50\n",
      "49696/49696 [==============================] - 44s 877us/step - loss: 0.4296 - acc: 0.8421 - val_loss: 0.4870 - val_acc: 0.8622\n",
      "Epoch 7/50\n",
      "49696/49696 [==============================] - 43s 875us/step - loss: 0.4092 - acc: 0.8495 - val_loss: 0.4817 - val_acc: 0.8708\n",
      "Epoch 8/50\n",
      "49696/49696 [==============================] - 44s 876us/step - loss: 0.3768 - acc: 0.8628 - val_loss: 0.5014 - val_acc: 0.8739\n",
      "Epoch 9/50\n",
      "49696/49696 [==============================] - 43s 875us/step - loss: 0.3515 - acc: 0.8730 - val_loss: 0.5086 - val_acc: 0.8712\n",
      "Epoch 10/50\n",
      "49696/49696 [==============================] - 43s 872us/step - loss: 0.3371 - acc: 0.8789 - val_loss: 0.5068 - val_acc: 0.8698\n",
      "Epoch 11/50\n",
      "49696/49696 [==============================] - 44s 882us/step - loss: 0.3187 - acc: 0.8872 - val_loss: 0.5375 - val_acc: 0.8763\n",
      "Epoch 12/50\n",
      "49696/49696 [==============================] - 44s 876us/step - loss: 0.3045 - acc: 0.8935 - val_loss: 0.4800 - val_acc: 0.8844\n",
      "Epoch 13/50\n",
      "49696/49696 [==============================] - 43s 873us/step - loss: 0.2874 - acc: 0.8986 - val_loss: 0.5153 - val_acc: 0.8721\n",
      "Epoch 14/50\n",
      "49696/49696 [==============================] - 43s 869us/step - loss: 0.2813 - acc: 0.9034 - val_loss: 0.5120 - val_acc: 0.8796\n",
      "Epoch 15/50\n",
      "49696/49696 [==============================] - 44s 893us/step - loss: 0.2633 - acc: 0.9077 - val_loss: 0.5404 - val_acc: 0.8642\n",
      "Epoch 16/50\n",
      "49696/49696 [==============================] - 44s 891us/step - loss: 0.2484 - acc: 0.9122 - val_loss: 0.5560 - val_acc: 0.8824\n",
      "Epoch 17/50\n",
      "49696/49696 [==============================] - 45s 899us/step - loss: 0.2361 - acc: 0.9171 - val_loss: 0.4876 - val_acc: 0.8924\n",
      "Epoch 18/50\n",
      "49696/49696 [==============================] - 44s 882us/step - loss: 0.2353 - acc: 0.9186 - val_loss: 0.4869 - val_acc: 0.8921\n",
      "Epoch 19/50\n",
      "49696/49696 [==============================] - 45s 899us/step - loss: 0.2237 - acc: 0.9221 - val_loss: 0.5609 - val_acc: 0.8822\n",
      "Epoch 20/50\n",
      "49696/49696 [==============================] - 43s 867us/step - loss: 0.2158 - acc: 0.9248 - val_loss: 0.5396 - val_acc: 0.8816\n",
      "Epoch 21/50\n",
      "49696/49696 [==============================] - 43s 874us/step - loss: 0.2109 - acc: 0.9263 - val_loss: 0.5870 - val_acc: 0.8798\n",
      "Epoch 22/50\n",
      "49696/49696 [==============================] - 43s 865us/step - loss: 0.2049 - acc: 0.9280 - val_loss: 0.5329 - val_acc: 0.8891\n",
      "Epoch 23/50\n",
      "49696/49696 [==============================] - 44s 884us/step - loss: 0.1951 - acc: 0.9318 - val_loss: 0.5411 - val_acc: 0.8926\n",
      "Epoch 24/50\n",
      "49696/49696 [==============================] - 45s 908us/step - loss: 0.1926 - acc: 0.9327 - val_loss: 0.6050 - val_acc: 0.8890\n",
      "Epoch 25/50\n",
      "49696/49696 [==============================] - 43s 866us/step - loss: 0.1858 - acc: 0.9347 - val_loss: 0.5796 - val_acc: 0.8824\n",
      "Epoch 26/50\n",
      "49696/49696 [==============================] - 43s 869us/step - loss: 0.1779 - acc: 0.9385 - val_loss: 0.5974 - val_acc: 0.8750\n",
      "Epoch 27/50\n",
      "49696/49696 [==============================] - 43s 862us/step - loss: 0.1754 - acc: 0.9389 - val_loss: 0.6441 - val_acc: 0.8788\n",
      "Epoch 28/50\n",
      "49696/49696 [==============================] - 43s 872us/step - loss: 0.1718 - acc: 0.9409 - val_loss: 0.6494 - val_acc: 0.8645\n",
      "Epoch 29/50\n",
      "49696/49696 [==============================] - 44s 890us/step - loss: 0.1669 - acc: 0.9427 - val_loss: 0.6263 - val_acc: 0.8716\n",
      "Epoch 30/50\n",
      "49696/49696 [==============================] - 44s 886us/step - loss: 0.1659 - acc: 0.9433 - val_loss: 0.6286 - val_acc: 0.8821\n",
      "Epoch 31/50\n",
      "49696/49696 [==============================] - 44s 884us/step - loss: 0.1620 - acc: 0.9442 - val_loss: 0.6588 - val_acc: 0.8698\n",
      "Epoch 32/50\n",
      "49696/49696 [==============================] - 43s 871us/step - loss: 0.1501 - acc: 0.9483 - val_loss: 0.6622 - val_acc: 0.8758\n",
      "Epoch 33/50\n",
      "49696/49696 [==============================] - 43s 867us/step - loss: 0.1585 - acc: 0.9450 - val_loss: 0.7073 - val_acc: 0.8606\n",
      "Epoch 34/50\n",
      "49696/49696 [==============================] - 43s 869us/step - loss: 0.1506 - acc: 0.9482 - val_loss: 0.6062 - val_acc: 0.8865\n",
      "Epoch 35/50\n",
      "49696/49696 [==============================] - 43s 874us/step - loss: 0.1457 - acc: 0.9503 - val_loss: 0.7375 - val_acc: 0.8650\n",
      "Epoch 36/50\n",
      "49696/49696 [==============================] - 44s 885us/step - loss: 0.1482 - acc: 0.9496 - val_loss: 0.6680 - val_acc: 0.8795\n",
      "Epoch 37/50\n",
      "14200/49696 [=======>......................] - ETA: 29s - loss: 0.1425 - acc: 0.9518"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "train_epoches = 50\n",
    "\n",
    "model.fit(reshaped_train,train_labels,validation_data=(reshaped_val,val_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(reshaped_test,test_labels,verbose=1)\n",
    "print(score)\n",
    "\n",
    "train_epoches = 20\n",
    "all_train = np.concatenate((reshaped_train, reshaped_val))\n",
    "all_labels = np.concatenate((train_labels, val_labels))\n",
    "model.fit(all_train,all_labels,validation_data=(reshaped_test,test_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(reshaped_test,test_labels,verbose=1)\n",
    "print(score)\n",
    "model.save('taskB2_all_Subjects_CNN_LSTM_overall.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(reshaped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F1-score measure\n",
    "from sklearn.metrics import f1_score\n",
    "num_classes = 18\n",
    "class_predictions = []\n",
    "class_true = []\n",
    "tot_labels = 0.0\n",
    "count = 0.0\n",
    "for pair in zip(predictions, test_labels):\n",
    "    class_predictions.append(np.argmax(pair[0]))\n",
    "    class_true.append(np.argmax(pair[1]))\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    tot_labels += 1.0\n",
    "    \n",
    "print('Standard accuracy is ' + str(count/tot_labels))    \n",
    "\n",
    "unique, counts = np.unique(class_true, return_counts=True)\n",
    "counted_labels = dict(zip(unique, counts))\n",
    "f1_scores = f1_score(class_predictions, class_true, average=None)\n",
    "\n",
    "tot_f1_score = 0.0\n",
    "weights_sum = 0.0\n",
    "for i in range(num_classes):\n",
    "    labels_class_i = counted_labels[i]\n",
    "    weight_i = labels_class_i / tot_labels\n",
    "    weights_sum += weight_i\n",
    "    tot_f1_score += f1_scores[i]*weight_i\n",
    "    print(str(i) + ' ' + str(weight_i) + ' ' + str(f1_scores[i]))\n",
    "\n",
    "    \n",
    "print('The weigths sum is ' + str(weights_sum))\n",
    "print('The computed f1-score is {}'.format(tot_f1_score))\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(class_true, class_predictions, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "layer_name = 'dense_layer'\n",
    "inter_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "network_output = inter_layer_model.predict(all_train)\n",
    "\n",
    "# PCA with certain number of principal components\n",
    "pca = PCA(n_components=40)\n",
    "pca.fit(network_output)\n",
    "new_output = pca.transform(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "svm_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# train a one-vs-one multi-class support vector machine\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(new_output, svm_labels)\n",
    "\n",
    "# predict test data\n",
    "svm_pred = clf.predict(pca.transform(inter_layer_model.predict(reshaped_test)))\n",
    "\n",
    "# measure accuracy and f1-score\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "new_test_labels = np.argmax(test_labels, axis=1)\n",
    "for pair in zip(svm_pred, new_test_labels):\n",
    "    if pair[0] == pair[1]:\n",
    "        num += 1.0\n",
    "    \n",
    "    den += 1.0\n",
    "\n",
    "scores = cross_val_score(clf, pca.transform(inter_layer_model.predict(reshaped_test)),new_test_labels, cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "print('Test accuracy is: {}'.format(num / den))\n",
    "f1_scores = f1_score(svm_pred, new_test_labels, average=None)\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(new_test_labels, svm_pred, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "predicted = cross_val_predict(clf, pca.transform(inter_layer_model.predict(reshaped_test)),new_test_labels, cv=5)\n",
    "\n",
    "# measure accuracy and f1-score\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "new_test_labels = np.argmax(test_labels, axis=1)\n",
    "for pair in zip(predicted, new_test_labels):\n",
    "    if pair[0] == pair[1]:\n",
    "        num += 1.0\n",
    "    \n",
    "    den += 1.0\n",
    "\n",
    "print('Test accuracy is: {}'.format(num / den))\n",
    "f1_scores = f1_score(predicted, new_test_labels, average=None)\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(new_test_labels, predicted, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.to_csv('preds_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(testY)\n",
    "true_df.to_csv('true_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
