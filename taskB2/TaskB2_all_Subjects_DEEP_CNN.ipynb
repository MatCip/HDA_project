{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as cp\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Conv2D, LSTM, CuDNNLSTM, Flatten, Dropout, Input, TimeDistributed, Reshape\n",
    "from keras import optimizers\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Activation, Dense, MaxPooling2D\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindow(sequence, labels, winSize, step, noNull):\n",
    "\n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # number of chunks\n",
    "    numOfChunks = ((len(sequence)-winSize)//step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        segment = sequence[i:i+winSize]\n",
    "        seg_labels = labels[i:i+winSize]\n",
    "        if noNull:\n",
    "            if seg_labels[-1] != 0:\n",
    "                yield segment, seg_labels\n",
    "        else:\n",
    "            yield segment, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_data(X_train, y_train, X_val, y_val, X_test, y_test, winSize, step, noNull=False):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    assert len(X_test) == len(y_test)\n",
    "    # obtain chunks of data\n",
    "    train_chunks = slidingWindow(X_train, y_train , winSize, step, noNull)\n",
    "    val_chunks = slidingWindow(X_val, y_val, winSize, step, noNull)\n",
    "    test_chunks = slidingWindow(X_test, y_test, winSize, step, noNull)\n",
    "    \n",
    "    # segment the data\n",
    "    train_segments = []\n",
    "    train_labels = []\n",
    "    for chunk in train_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        train_segments.append(data)\n",
    "        train_labels.append(labels[-1])\n",
    "        \n",
    "    val_segments = []\n",
    "    val_labels = []\n",
    "    for chunk in val_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        val_segments.append(data)\n",
    "        val_labels.append(labels[-1])\n",
    "    \n",
    "    test_segments = []\n",
    "    test_labels = []\n",
    "    for chunk in test_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        test_segments.append(data)\n",
    "        test_labels.append(labels[-1])\n",
    "        \n",
    "    return np.array(train_segments), np.array(train_labels), np.array(val_segments), np.array(val_labels), np.array(test_segments), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, test_data):\n",
    "    encoder = OneHotEncoder()\n",
    "    train_labels = encoder.fit_transform(train_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    val_labels = encoder.transform(val_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    test_labels = encoder.transform(test_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data.drop(['labels'], axis=1, inplace=True)\n",
    "    val_data.drop(['labels'], axis=1, inplace=True)\n",
    "    test_data.drop(['labels'], axis=1, inplace=True)\n",
    "    data = pd.concat([train_data,val_data,test_data])\n",
    "    scaler.fit(data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import train data\n",
    "adl_1_1 = pd.read_csv(\"ADL1Opportunity_locomotion_S1.csv\",header=None)\n",
    "adl_1_2 = pd.read_csv(\"ADL2Opportunity_locomotion_S1.csv\",header=None)\n",
    "adl_1_3 = pd.read_csv(\"ADL3Opportunity_locomotion_S1.csv\",header=None)\n",
    "adl_1_4 = pd.read_csv(\"ADL4Opportunity_locomotion_S1.csv\",header=None)\n",
    "adl_1_5 = pd.read_csv(\"ADL5Opportunity_locomotion_S1.csv\",header=None)\n",
    "drill_1 = pd.read_csv(\"Drill1Opportunity_locomotion.csv\",header=None)\n",
    "adl_2_1 = pd.read_csv(\"ADL1Opportunity_locomotion_S2.csv\",header=None)\n",
    "adl_2_2 = pd.read_csv(\"ADL2Opportunity_locomotion_S2.csv\",header=None)\n",
    "drill_2 = pd.read_csv(\"Drill2Opportunity_locomotion.csv\",header=None)\n",
    "adl_3_1 = pd.read_csv(\"ADL1Opportunity_locomotion_S3.csv\",header=None)\n",
    "adl_3_2 = pd.read_csv(\"ADL2Opportunity_locomotion_S3.csv\",header=None)\n",
    "drill_3 = pd.read_csv(\"Drill3Opportunity_locomotion.csv\",header=None)\n",
    "\n",
    "# import validation data\n",
    "adl_2_3 = pd.read_csv(\"ADL3Opportunity_locomotion_S2.csv\",header=None)\n",
    "adl_3_3 = pd.read_csv(\"ADL3Opportunity_locomotion_S3.csv\",header=None)\n",
    "\n",
    "# import test data\n",
    "adl_2_4 = pd.read_csv(\"ADL4Opportunity_locomotion_S3.csv\",header=None)\n",
    "adl_2_5 = pd.read_csv(\"ADL5Opportunity_locomotion_S3.csv\",header=None)\n",
    "adl_3_4 = pd.read_csv(\"ADL4Opportunity_locomotion_S3.csv\",header=None)\n",
    "adl_3_5 = pd.read_csv(\"ADL5Opportunity_locomotion_S3.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data is (495691, 114)\n",
      "Shape of test data is (108352, 114)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148.0</td>\n",
       "      <td>956.0</td>\n",
       "      <td>-358.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>986.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>...</td>\n",
       "      <td>319.0</td>\n",
       "      <td>-845.0</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>-287.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>968.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>...</td>\n",
       "      <td>325.0</td>\n",
       "      <td>-847.0</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>-418.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>...</td>\n",
       "      <td>328.0</td>\n",
       "      <td>-852.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>-425.0</td>\n",
       "      <td>-47.0</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>353.0</td>\n",
       "      <td>...</td>\n",
       "      <td>321.0</td>\n",
       "      <td>-852.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>-26.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64.0</td>\n",
       "      <td>857.0</td>\n",
       "      <td>-391.0</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>1022.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>548.0</td>\n",
       "      <td>...</td>\n",
       "      <td>321.0</td>\n",
       "      <td>-850.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 114 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2     3       4      5      6       7      8      9  \\\n",
       "0  148.0  956.0 -358.0  19.0   986.0  196.0   95.0   975.0  152.0  194.0   \n",
       "1   89.0  973.0 -287.0  10.0  1004.0  162.0  125.0   968.0  122.0  224.0   \n",
       "2  130.0  988.0 -418.0 -11.0  1014.0  202.0  127.0  1002.0  113.0  279.0   \n",
       "3   89.0  980.0 -425.0 -47.0  1025.0  191.0  110.0  1006.0  105.0  353.0   \n",
       "4   64.0  857.0 -391.0  -8.0  1022.0  204.0   97.0  1002.0   93.0  548.0   \n",
       "\n",
       "    ...      104    105   106   107   108   109   110   111    112  labels  \n",
       "0   ...    319.0 -845.0 -20.0  57.0  42.0  57.0  20.0  42.0  175.0       1  \n",
       "1   ...    325.0 -847.0 -17.0  38.0  31.0  38.0  17.0  31.0  175.0       1  \n",
       "2   ...    328.0 -852.0  27.0  31.0  15.0  31.0 -27.0  15.0  175.0       1  \n",
       "3   ...    321.0 -852.0  26.0  22.0  -2.0  22.0 -26.0  -2.0  175.0       1  \n",
       "4   ...    321.0 -850.0  22.0  45.0  -7.0  45.0 -22.0  -7.0  175.0       1  \n",
       "\n",
       "[5 rows x 114 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frames = [adl_1_1,adl_1_2,adl_1_3,adl_1_4,adl_1_5,drill_1,adl_2_1,adl_2_2,drill_2,adl_3_1,adl_3_2,drill_3]\n",
    "val_frames = [adl_2_3,adl_3_3]\n",
    "test_frames = [adl_2_4,adl_2_5,adl_3_4,adl_3_5]\n",
    "train_data = pd.concat(train_frames)\n",
    "val_data = pd.concat(val_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "train_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "val_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "test_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "print('Shape of train data is {}'.format(train_data.shape))\n",
    "print('Shape of test data is {}'.format(test_data.shape))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train, scaled_val, scaled_test, train_labels, val_labels, test_labels = prepare_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(495691, 113)\n",
      "(495691, 5)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sensors = 113\n",
    "window_size = 30\n",
    "step_size = 10\n",
    "classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_segments, train_labels, val_segments, val_labels, test_segments, test_labels = segment_data(scaled_train, train_labels, scaled_val, val_labels,\n",
    "                                                                                                  scaled_test, test_labels, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49567, 30, 113)\n"
     ]
    }
   ],
   "source": [
    "print(train_segments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segments = train_segments.reshape((-1,num_sensors, window_size,1))\n",
    "test_segments = test_segments.reshape((-1,num_sensors, window_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49567, 113, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_segments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "class ConcatenationLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super(ConcatenationLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.concatenating_weights = self.add_weight(name='kernel', \n",
    "                                      shape=(113,20),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        \n",
    "\n",
    "    def call(self, x):\n",
    "    \n",
    "       # for i in range(0,50):\n",
    "        #    w[:,:,:,i]=tf.multiply(x[:,:,:,i],coeff_norm)\n",
    "        x_reshaped=tf.concat(x,axis=1)\n",
    "        \n",
    "        next_layer=tf.tanh(tf.matmul(tf.matmul(x_reshaped,self.concatenating_weights),tf.ones(1,20)))\n",
    "        \n",
    "\n",
    "        return next_layer\n",
    "    \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "         return output_dim\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class NormalizationLayer(Layer):\n",
    "\n",
    "    def __init__(self, output_dim,**kwargs):\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        super(NormalizationLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.alpha = self.add_weight(name='alpha', \n",
    "                                      shape=(1,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', \n",
    "                                      shape=(1,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        self.k = self.add_weight(name='k', \n",
    "                                      shape=(1,1),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "        super(NormalizationLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, x):\n",
    "        layer_squared = K.square(x)\n",
    "        coeff_norm=K.pow((self.k+self.alpha*K.sum(layer_squared, axis=3)),-self.beta)\n",
    "        print(coeff_norm.shape)\n",
    "        \n",
    "\n",
    "        \n",
    "      \n",
    "       # for i in range(0,50):\n",
    "        #    w[:,:,:,i]=tf.multiply(x[:,:,:,i],coeff_norm)\n",
    "        x_unpacked = tf.unstack(x,axis=3)\n",
    "        print(x_unpacked[0])\n",
    "        processed=[]\n",
    "        for t in x_unpacked:\n",
    "        # do whatever\n",
    "            result_tensor=tf.multiply(t,coeff_norm)\n",
    "            \n",
    "            processed.append(result_tensor)\n",
    "        output = tf.stack(processed, axis=3) \n",
    "        print(output.shape)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "         return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "size_of_kernel_1_2 = (1,5)\n",
    "size_of_kernel_3= (1,3)\n",
    "kernel_strides = 1\n",
    "num_filters_1_conv_layer = 50\n",
    "num_filters_2_conv_layer = 40\n",
    "num_filters_3_conv_layer = 20\n",
    "dropout_prob = 0\n",
    "inputshape = (num_sensors, window_size,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "(?, 113, 13)\n",
      "Tensor(\"normalization_layer1/unstack:0\", shape=(?, 113, 13), dtype=float32)\n",
      "(?, 113, 13, 50)\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(num_filters_1_conv_layer, kernel_size=size_of_kernel_1_2, strides=kernel_strides,\n",
    "                 activation='tanh', input_shape=inputshape, name='1_conv_layer'))\n",
    "\n",
    "model.add(Activation('relu',name='relu1'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 2), strides=None, padding='valid', data_format=None,name='maxpooling1'))\n",
    "model.add(NormalizationLayer(output_dim=model.layers[-1].input_shape,name='normalization_layer1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 113, 3)\n",
      "Tensor(\"normalization_layer2/unstack:0\", shape=(?, 113, 3), dtype=float32)\n",
      "(?, 113, 3, 40)\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(num_filters_2_conv_layer, kernel_size=size_of_kernel_1_2, strides=kernel_strides,\n",
    "                 activation='tanh', input_shape=inputshape, name='2_conv_layer'))\n",
    "\n",
    "\n",
    "model.add(Activation('relu',name='relu2'))\n",
    "model.add(MaxPooling2D(pool_size=(1, 3), strides=None, padding='valid', data_format=None,name='maxpooling2'))\n",
    "model.add(NormalizationLayer(output_dim=model.layers[-1].input_shape,name='normalization_layer2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters_3_conv_layer, kernel_size=size_of_kernel_3, strides=kernel_strides,\n",
    "                 activation='tanh', input_shape=inputshape, name='3_conv_layer'))\n",
    "\n",
    "model.add(Activation('relu',name='relu3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=400, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None,name='1_dense_layer'))\n",
    "model.add(Activation('relu',name='1_dense_relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.add(Reshape((400,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(classes, activation='softmax',name='softmax_layer'))\n",
    "rms = optimizers.RMSprop(lr=0.001, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rms, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_conv_layer: input shape: (None, 113, 30, 1) output shape: (None, 113, 26, 50)\n",
      "relu1: input shape: (None, 113, 26, 50) output shape: (None, 113, 26, 50)\n",
      "maxpooling1: input shape: (None, 113, 26, 50) output shape: (None, 113, 13, 50)\n",
      "normalization_layer1: input shape: (None, 113, 13, 50) output shape: (None, 113, 13, 50)\n",
      "2_conv_layer: input shape: (None, 113, 13, 50) output shape: (None, 113, 9, 40)\n",
      "relu2: input shape: (None, 113, 9, 40) output shape: (None, 113, 9, 40)\n",
      "maxpooling2: input shape: (None, 113, 9, 40) output shape: (None, 113, 3, 40)\n",
      "normalization_layer2: input shape: (None, 113, 3, 40) output shape: (None, 113, 3, 40)\n",
      "3_conv_layer: input shape: (None, 113, 3, 40) output shape: (None, 113, 1, 20)\n",
      "relu3: input shape: (None, 113, 1, 20) output shape: (None, 113, 1, 20)\n",
      "flatten_1: input shape: (None, 113, 1, 20) output shape: (None, 2260)\n",
      "1_dense_layer: input shape: (None, 2260) output shape: (None, 400)\n",
      "1_dense_relu: input shape: (None, 400) output shape: (None, 400)\n",
      "softmax_layer: input shape: (None, 400) output shape: (None, 5)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(str(layer.name) + ': input shape: ' + str(layer.input_shape) + ' output shape: ' + str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "1_conv_layer (Conv2D)        (None, 113, 26, 50)       300       \n",
      "_________________________________________________________________\n",
      "relu1 (Activation)           (None, 113, 26, 50)       0         \n",
      "_________________________________________________________________\n",
      "maxpooling1 (MaxPooling2D)   (None, 113, 13, 50)       0         \n",
      "_________________________________________________________________\n",
      "normalization_layer1 (Normal (None, 113, 13, 50)       3         \n",
      "_________________________________________________________________\n",
      "2_conv_layer (Conv2D)        (None, 113, 9, 40)        10040     \n",
      "_________________________________________________________________\n",
      "relu2 (Activation)           (None, 113, 9, 40)        0         \n",
      "_________________________________________________________________\n",
      "maxpooling2 (MaxPooling2D)   (None, 113, 3, 40)        0         \n",
      "_________________________________________________________________\n",
      "normalization_layer2 (Normal (None, 113, 3, 40)        3         \n",
      "_________________________________________________________________\n",
      "3_conv_layer (Conv2D)        (None, 113, 1, 20)        2420      \n",
      "_________________________________________________________________\n",
      "relu3 (Activation)           (None, 113, 1, 20)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2260)              0         \n",
      "_________________________________________________________________\n",
      "1_dense_layer (Dense)        (None, 400)               904400    \n",
      "_________________________________________________________________\n",
      "1_dense_relu (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "softmax_layer (Dense)        (None, 5)                 2005      \n",
      "=================================================================\n",
      "Total params: 919,171\n",
      "Trainable params: 919,171\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49567 samples, validate on 10833 samples\n",
      "Epoch 1/50\n",
      "49567/49567 [==============================] - 47s 945us/step - loss: 1.5162 - acc: 0.4224 - val_loss: 1.5035 - val_acc: 0.3582\n",
      "Epoch 2/50\n",
      "49567/49567 [==============================] - 42s 840us/step - loss: 1.4179 - acc: 0.4231 - val_loss: 1.4908 - val_acc: 0.3582\n",
      "Epoch 3/50\n",
      "49567/49567 [==============================] - 42s 848us/step - loss: 1.3877 - acc: 0.4231 - val_loss: 1.4981 - val_acc: 0.3582\n",
      "Epoch 4/50\n",
      "49567/49567 [==============================] - 42s 849us/step - loss: 1.3792 - acc: 0.4231 - val_loss: 1.5074 - val_acc: 0.3582\n",
      "Epoch 5/50\n",
      "49567/49567 [==============================] - 42s 850us/step - loss: 1.3773 - acc: 0.4231 - val_loss: 1.5131 - val_acc: 0.3582\n",
      "Epoch 6/50\n",
      "49567/49567 [==============================] - 42s 857us/step - loss: 1.3769 - acc: 0.4231 - val_loss: 1.5156 - val_acc: 0.3582\n",
      "Epoch 7/50\n",
      "49567/49567 [==============================] - 42s 855us/step - loss: 1.3768 - acc: 0.4231 - val_loss: 1.5176 - val_acc: 0.3582\n",
      "Epoch 8/50\n",
      "  500/49567 [..............................] - ETA: 38s - loss: 1.3353 - acc: 0.4540"
     ]
    }
   ],
   "source": [
    "batchSize = 100\n",
    "train_epoches = 50\n",
    "model.fit(train_segments,train_labels,validation_data=(test_segments,test_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(test_segments,test_labels,verbose=1)\n",
    "print(score)\n",
    "model.save('taskA_all_Subjects_DEEP_CNN_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-score measure\n",
    "from sklearn.metrics import f1_score\n",
    "num_classes = 18\n",
    "class_predictions = []\n",
    "class_true = []\n",
    "tot_labels = 0.0\n",
    "count = 0.0\n",
    "for pair in zip(predictions, test_labels):\n",
    "    class_predictions.append(np.argmax(pair[0]))\n",
    "    class_true.append(np.argmax(pair[1]))\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    tot_labels += 1.0\n",
    "    \n",
    "print('Standard accuracy is ' + str(count/tot_labels))    \n",
    "\n",
    "unique, counts = np.unique(class_true, return_counts=True)\n",
    "counted_labels = dict(zip(unique, counts))\n",
    "f1_scores = f1_score(class_predictions, class_true, average=None)\n",
    "\n",
    "tot_f1_score = 0.0\n",
    "weights_sum = 0.0\n",
    "for i in range(num_classes):\n",
    "    labels_class_i = counted_labels[i]\n",
    "    weight_i = labels_class_i / tot_labels\n",
    "    weights_sum += weight_i\n",
    "    tot_f1_score += f1_scores[i]*weight_i\n",
    "    \n",
    "print('The computed f1-score is {}'.format(tot_f1_score))\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(class_true, class_predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.to_csv('preds_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(testY)\n",
    "true_df.to_csv('true_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
