{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, LSTM, CuDNNLSTM, Flatten, Dropout, Reshape, Flatten, BatchNormalization, MaxPooling2D\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindow(sequence, labels, winSize, step, noNull):\n",
    "\n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # number of chunks\n",
    "    numOfChunks = ((len(sequence)-winSize)//step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        segment = sequence[i:i+winSize]\n",
    "        seg_labels = labels[i:i+winSize]\n",
    "        if noNull:\n",
    "            if seg_labels[-1] != 0:\n",
    "                yield segment, seg_labels\n",
    "        else:\n",
    "            yield segment, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_data(X_train, y_train, X_val, y_val, X_test, y_test, winSize, step, noNull=False):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    assert len(X_test) == len(y_test)\n",
    "    # obtain chunks of data\n",
    "    train_chunks = slidingWindow(X_train, y_train , winSize, step, noNull)\n",
    "    val_chunks = slidingWindow(X_val, y_val, winSize, step, noNull)\n",
    "    test_chunks = slidingWindow(X_test, y_test, winSize, step, noNull)\n",
    "    \n",
    "    # segment the data\n",
    "    train_segments = []\n",
    "    train_labels = []\n",
    "    for chunk in train_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        train_segments.append(data)\n",
    "        train_labels.append(labels[-1])\n",
    "        \n",
    "    val_segments = []\n",
    "    val_labels = []\n",
    "    for chunk in val_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        val_segments.append(data)\n",
    "        val_labels.append(labels[-1])\n",
    "    \n",
    "    test_segments = []\n",
    "    test_labels = []\n",
    "    for chunk in test_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        test_segments.append(data)\n",
    "        test_labels.append(labels[-1])\n",
    "        \n",
    "    return np.array(train_segments), np.array(train_labels), np.array(val_segments), np.array(val_labels), np.array(test_segments), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, test_data):\n",
    "    encoder = OneHotEncoder()\n",
    "    train_labels = encoder.fit_transform(train_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    val_labels = encoder.transform(val_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    test_labels = encoder.transform(test_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data.drop(['labels'], axis=1, inplace=True)\n",
    "    val_data.drop(['labels'], axis=1, inplace=True)\n",
    "    test_data.drop(['labels'], axis=1, inplace=True)\n",
    "    data = pd.concat([train_data,val_data,test_data])\n",
    "    scaler.fit(data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train data\n",
    "adl_1_1 = pd.read_csv(\"./reduced_dataset/ACC/ADL1Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_2 = pd.read_csv(\"./reduced_dataset/ACC/ADL2Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_3 = pd.read_csv(\"./reduced_dataset/ACC/ADL3Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_4 = pd.read_csv(\"./reduced_dataset/ACC/ADL4Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_5 = pd.read_csv(\"./reduced_dataset/ACC/ADL5Opportunity_taskB2_S1.csv\",header=None)\n",
    "drill_1 = pd.read_csv(\"./reduced_dataset/ACC/Drill1Opportunity_taskB2.csv\",header=None)\n",
    "adl_2_1 = pd.read_csv(\"./reduced_dataset/ACC/ADL1Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_2 = pd.read_csv(\"./reduced_dataset/ACC/ADL2Opportunity_taskB2_S2.csv\",header=None)\n",
    "drill_2 = pd.read_csv(\"./reduced_dataset/ACC/Drill2Opportunity_taskB2.csv\",header=None)\n",
    "adl_3_1 = pd.read_csv(\"./reduced_dataset/ACC/ADL1Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_2 = pd.read_csv(\"./reduced_dataset/ACC/ADL2Opportunity_taskB2_S3.csv\",header=None)\n",
    "drill_3 = pd.read_csv(\"./reduced_dataset/ACC/Drill3Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "# import validation data\n",
    "adl_2_3 = pd.read_csv(\"./reduced_dataset/ACC/ADL3Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_3 = pd.read_csv(\"./reduced_dataset/ACC/ADL3Opportunity_taskB2_S3.csv\",header=None)\n",
    "\n",
    "# import test data\n",
    "adl_2_4 = pd.read_csv(\"./reduced_dataset/ACC/ADL4Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_5 = pd.read_csv(\"./reduced_dataset/ACC/ADL5Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_4 = pd.read_csv(\"./reduced_dataset/ACC/ADL4Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_5 = pd.read_csv(\"./reduced_dataset/ACC/ADL5Opportunity_taskB2_S3.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frames = [adl_1_1,adl_1_2,adl_1_3,adl_1_4,adl_1_5,drill_1,adl_2_1,adl_2_2,drill_2,adl_3_1,adl_3_2,drill_3]\n",
    "val_frames = [adl_2_3,adl_3_3]\n",
    "test_frames = [adl_2_4,adl_2_5,adl_3_4,adl_3_5]\n",
    "train_data = pd.concat(train_frames)\n",
    "val_data = pd.concat(val_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "train_data.rename(columns ={9: 'labels'}, inplace =True)\n",
    "val_data.rename(columns ={9: 'labels'}, inplace =True)\n",
    "test_data.rename(columns ={9: 'labels'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train (497014, 10), val (60949, 10), test (118750, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"shapes: train {0}, val {1}, test {2}\".format(train_data.shape, val_data.shape, test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train, scaled_val, scaled_test, train_labels, val_labels, test_labels = prepare_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497014, 9)\n",
      "(497014, 18)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sensors = 9\n",
    "window_size = 24\n",
    "step_size = 12\n",
    "classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segments, train_labels, val_segments, val_labels, test_segments, test_labels = segment_data(scaled_train, train_labels, scaled_val, val_labels,\n",
    "                                                                                                  scaled_test, test_labels, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input for CNN\n",
    "reshaped_train = train_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_val = val_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_test = test_segments.reshape(-1, window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "inputshape = (window_size, num_sensors, 1)\n",
    "\n",
    "model.add(Flatten(input_shape=inputshape))\n",
    "\n",
    "model.add(Dense(128,kernel_initializer='glorot_normal', activation='tanh',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer_1'))\n",
    "\n",
    "model.add(Dropout(0.5, name='1_dropout_layer'))\n",
    "\n",
    "model.add(Dense(32,kernel_initializer='glorot_normal', activation='tanh',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer_2'))\n",
    "\n",
    "model.add(Dropout(0.5, name='2_dropout_layer'))\n",
    "\n",
    "model.add(Dense(128,kernel_initializer='glorot_normal', activation='tanh',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer_3'))\n",
    "\n",
    "model.add(Dropout(0.5, name='3_dropout_layer'))\n",
    "\n",
    "model.add(Dense(216,kernel_initializer='glorot_normal', activation='sigmoid',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer_4'))\n",
    "model.add(Reshape((window_size, num_sensors,1)))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "dropout_prob = 0.8\n",
    "inputshape = (window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=(5,1), strides=1,\n",
    "                 kernel_initializer='glorot_normal', name='1_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=(5,1), strides=1,\n",
    "                 kernel_initializer='glorot_normal', name='2_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(20, kernel_size=(5,3), strides=(1,3),\n",
    "                 activation='tanh',\n",
    "                 kernel_initializer='glorot_normal', name='3_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(128,kernel_initializer='glorot_normal', activation='tanh',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer_1'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='1_dropout_layer'))\n",
    "\n",
    "model.add(Dense(classes,kernel_initializer='glorot_normal', activation='tanh',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='red_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='2_dropout_layer'))\n",
    "\n",
    "model.add(Dense(128,kernel_initializer='glorot_normal', activation='tanh',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer_2'))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='mean_squared_error', optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatten_14: input shape: (None, 24, 9, 1) output shape: (None, 216)\n",
      "dense_layer_1: input shape: (None, 216) output shape: (None, 128)\n",
      "1_dropout_layer: input shape: (None, 128) output shape: (None, 128)\n",
      "dense_layer_2: input shape: (None, 128) output shape: (None, 128)\n",
      "2_dropout_layer: input shape: (None, 128) output shape: (None, 128)\n",
      "dense_layer_3: input shape: (None, 128) output shape: (None, 128)\n",
      "3_dropout_layer: input shape: (None, 128) output shape: (None, 128)\n",
      "dense_layer_4: input shape: (None, 128) output shape: (None, 216)\n",
      "reshape_3: input shape: (None, 216) output shape: (None, 24, 9, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(str(layer.name) + ': input shape: ' + str(layer.input_shape) + ' output shape: ' + str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "41416/41416 [==============================] - 3s 72us/step - loss: 0.0087\n",
      "Epoch 2/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 0.0020\n",
      "Epoch 3/50\n",
      "41416/41416 [==============================] - 2s 47us/step - loss: 0.0015\n",
      "Epoch 4/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 0.0013\n",
      "Epoch 5/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 0.0012\n",
      "Epoch 6/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 0.0011\n",
      "Epoch 7/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 0.0011\n",
      "Epoch 8/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 0.0010\n",
      "Epoch 9/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 9.8557e-04\n",
      "Epoch 10/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 9.6347e-04\n",
      "Epoch 11/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 9.4376e-04\n",
      "Epoch 12/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 9.3314e-04\n",
      "Epoch 13/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 9.1474e-04\n",
      "Epoch 14/50\n",
      "41416/41416 [==============================] - 2s 47us/step - loss: 9.0808e-04\n",
      "Epoch 15/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.9641e-04\n",
      "Epoch 16/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.8750e-04\n",
      "Epoch 17/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 8.8305e-04\n",
      "Epoch 18/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 8.7620e-04\n",
      "Epoch 19/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 8.7034e-04\n",
      "Epoch 20/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.6522e-04\n",
      "Epoch 21/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.6105e-04\n",
      "Epoch 22/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 8.5632e-04\n",
      "Epoch 23/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.5123e-04\n",
      "Epoch 24/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 8.4535e-04\n",
      "Epoch 25/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.4118e-04\n",
      "Epoch 26/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.3777e-04\n",
      "Epoch 27/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.2758e-04\n",
      "Epoch 28/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.2486e-04\n",
      "Epoch 29/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 8.1714e-04\n",
      "Epoch 30/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 8.1576e-04\n",
      "Epoch 31/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 8.0848e-04\n",
      "Epoch 32/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 8.0575e-04\n",
      "Epoch 33/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.9704e-04\n",
      "Epoch 34/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 7.9173e-04\n",
      "Epoch 35/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 7.8989e-04\n",
      "Epoch 36/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 7.8116e-04\n",
      "Epoch 37/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 7.7784e-04\n",
      "Epoch 38/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 7.7071e-04\n",
      "Epoch 39/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.6445e-04\n",
      "Epoch 40/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 7.5920e-04\n",
      "Epoch 41/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 7.5448e-04\n",
      "Epoch 42/50\n",
      "41416/41416 [==============================] - 2s 44us/step - loss: 7.5300e-04\n",
      "Epoch 43/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.4730e-04\n",
      "Epoch 44/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.4280e-04\n",
      "Epoch 45/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 7.3943e-04\n",
      "Epoch 46/50\n",
      "41416/41416 [==============================] - 2s 45us/step - loss: 7.3245e-04\n",
      "Epoch 47/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.3134e-04\n",
      "Epoch 48/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.2416e-04\n",
      "Epoch 49/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.1806e-04\n",
      "Epoch 50/50\n",
      "41416/41416 [==============================] - 2s 46us/step - loss: 7.1309e-04\n",
      "Calculating score.. \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected reshape_4 to have 4 dimensions, but got array with shape (9894, 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-ce77d9f366c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Calculating score.. '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreshaped_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'taskB2_all_Subjects_CNN_reduced.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    920\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1679\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1681\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1682\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1413\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1414\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1415\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    140\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected reshape_4 to have 4 dimensions, but got array with shape (9894, 18)"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "train_epoches = 50\n",
    "\n",
    "model.fit(reshaped_train,reshaped_train,epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(reshaped_test,test_labels,verbose=1)\n",
    "print(score)\n",
    "model.save('taskB2_all_Subjects_CNN_reduced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "layer_name = 'dense_layer_2'\n",
    "inter_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "network_output = inter_layer_model.predict(reshaped_train)\n",
    "\n",
    "# PCA with certain number of principal components\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(network_output)\n",
    "new_output = pca.transform(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21964681 -0.34140134 -0.36596084 ..., -0.21540095 -0.01774176\n",
      "  -0.03950792]\n",
      " [ 0.21112008 -0.38408151 -0.40389311 ..., -0.20702547 -0.01994131\n",
      "  -0.02055739]\n",
      " [ 0.1735802  -0.40010458 -0.4214659  ..., -0.16527715 -0.00306433\n",
      "  -0.0029956 ]\n",
      " ..., \n",
      " [ 0.12283116  0.14538564  0.0953061  ..., -0.14268973 -0.05730413\n",
      "  -0.30291271]\n",
      " [ 0.13366476  0.13920215  0.08839852 ..., -0.15403502 -0.05867961\n",
      "  -0.30370635]\n",
      " [ 0.14445229  0.1330096   0.08148516 ..., -0.1653261  -0.06004905\n",
      "  -0.30449286]]\n"
     ]
    }
   ],
   "source": [
    "print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81231672  0.81630648  0.81710914  0.82195846  0.81962339]\n",
      "Accuracy: 0.82 (+/- 0.01)\n",
      "Test accuracy is: 0.8156754627806223\n",
      "The f1-score with sklearn function is 0.7349925970478752\n",
      "Average f1-score is 0.049915642323451435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\sklearn\\metrics\\classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "svm_labels = np.argmax(train_labels, axis=1)\n",
    "\n",
    "# train a one-vs-one multi-class support vector machine\n",
    "clf = svm.SVC(decision_function_shape='ovr')\n",
    "clf.fit(new_output, svm_labels)\n",
    "\n",
    "# predict test data\n",
    "svm_pred = clf.predict(pca.transform(inter_layer_model.predict(reshaped_val)))\n",
    "\n",
    "# measure accuracy and f1-score\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "new_test_labels = np.argmax(val_labels, axis=1)\n",
    "for pair in zip(svm_pred, new_test_labels):\n",
    "    if pair[0] == pair[1]:\n",
    "        num += 1.0\n",
    "    \n",
    "    den += 1.0\n",
    "\n",
    "scores = cross_val_score(clf, pca.transform(inter_layer_model.predict(reshaped_val)),new_test_labels, cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "print('Test accuracy is: {}'.format(num / den))\n",
    "f1_scores = f1_score(svm_pred, new_test_labels, average=None)\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(new_test_labels, svm_pred, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy is 0.8603193854861533\n",
      "0 0.832524762482 0.931251512951\n",
      "1 0.0058621386699 0.608695652174\n",
      "2 0.00960177885587 0.883720930233\n",
      "3 0.00606428138266 0.59872611465\n",
      "4 0.00838892257934 0.847457627119\n",
      "5 0.0230442692541 0.547738693467\n",
      "6 0.0161714170204 0.65671641791\n",
      "7 0.0101071356378 0.350364963504\n",
      "8 0.00778249444108 0.31884057971\n",
      "9 0.00394178289873 0.329411764706\n",
      "10 0.00424499696786 0.0338983050847\n",
      "11 0.0040428542551 0.230769230769\n",
      "12 0.00262785526582 0.0952380952381\n",
      "13 0.0067717808773 0.360248447205\n",
      "14 0.00616535273903 0.292134831461\n",
      "15 0.0100060642814 0.506666666667\n",
      "16 0.0320396199717 0.487951807229\n",
      "17 0.0106124924196 0.432989690722\n",
      "The weigths sum is 1.0\n",
      "The computed f1-score is 0.8595133036902902\n",
      "The f1-score with sklearn function is 0.85951330369029\n"
     ]
    }
   ],
   "source": [
    "# F1-score measure\n",
    "from sklearn.metrics import f1_score\n",
    "num_classes = 18\n",
    "class_predictions = []\n",
    "class_true = []\n",
    "tot_labels = 0.0\n",
    "count = 0.0\n",
    "for pair in zip(predictions, test_labels):\n",
    "    class_predictions.append(np.argmax(pair[0]))\n",
    "    class_true.append(np.argmax(pair[1]))\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    tot_labels += 1.0\n",
    "    \n",
    "print('Standard accuracy is ' + str(count/tot_labels))    \n",
    "\n",
    "unique, counts = np.unique(class_true, return_counts=True)\n",
    "counted_labels = dict(zip(unique, counts))\n",
    "f1_scores = f1_score(class_predictions, class_true, average=None)\n",
    "\n",
    "tot_f1_score = 0.0\n",
    "weights_sum = 0.0\n",
    "for i in range(num_classes):\n",
    "    labels_class_i = counted_labels[i]\n",
    "    weight_i = labels_class_i / tot_labels\n",
    "    weights_sum += weight_i\n",
    "    tot_f1_score += f1_scores[i]*weight_i\n",
    "    print(str(i) + ' ' + str(weight_i) + ' ' + str(f1_scores[i]))\n",
    "\n",
    "    \n",
    "print('The weigths sum is ' + str(weights_sum))\n",
    "print('The computed f1-score is {}'.format(tot_f1_score))\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(class_true, class_predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.to_csv('preds_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(testY)\n",
    "true_df.to_csv('true_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
