{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, LSTM, CuDNNLSTM, Flatten, Dropout, Reshape, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindow(sequence, labels, winSize, step, noNull):\n",
    "\n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # number of chunks\n",
    "    numOfChunks = ((len(sequence)-winSize)//step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        segment = sequence[i:i+winSize]\n",
    "        seg_labels = labels[i:i+winSize]\n",
    "        if noNull:\n",
    "            if seg_labels[-1] != 0:\n",
    "                yield segment, seg_labels\n",
    "        else:\n",
    "            yield segment, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_data(X_train, y_train, X_val, y_val, X_test, y_test, winSize, step, noNull=False):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    assert len(X_test) == len(y_test)\n",
    "    # obtain chunks of data\n",
    "    train_chunks = slidingWindow(X_train, y_train , winSize, step, noNull)\n",
    "    val_chunks = slidingWindow(X_val, y_val, winSize, step, noNull)\n",
    "    test_chunks = slidingWindow(X_test, y_test, winSize, step, noNull)\n",
    "    \n",
    "    # segment the data\n",
    "    train_segments = []\n",
    "    train_labels = []\n",
    "    for chunk in train_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        train_segments.append(data)\n",
    "        train_labels.append(labels[-1])\n",
    "        \n",
    "    val_segments = []\n",
    "    val_labels = []\n",
    "    for chunk in val_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        val_segments.append(data)\n",
    "        val_labels.append(labels[-1])\n",
    "    \n",
    "    test_segments = []\n",
    "    test_labels = []\n",
    "    for chunk in test_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        test_segments.append(data)\n",
    "        test_labels.append(labels[-1])\n",
    "        \n",
    "    return np.array(train_segments), np.array(train_labels), np.array(val_segments), np.array(val_labels), np.array(test_segments), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, test_data):\n",
    "    encoder = OneHotEncoder()\n",
    "    train_labels = encoder.fit_transform(train_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    val_labels = encoder.transform(val_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    test_labels = encoder.transform(test_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data.drop(['labels'], axis=1, inplace=True)\n",
    "    val_data.drop(['labels'], axis=1, inplace=True)\n",
    "    test_data.drop(['labels'], axis=1, inplace=True)\n",
    "    data = pd.concat([train_data,val_data,test_data])\n",
    "    scaler.fit(data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import train data\n",
    "adl_1_1 = pd.read_csv(\"./reduced_dataset/ADL1Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_2 = pd.read_csv(\"./reduced_dataset/ADL2Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_3 = pd.read_csv(\"./reduced_dataset/ADL3Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_4 = pd.read_csv(\"./reduced_dataset/ADL4Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_5 = pd.read_csv(\"./reduced_dataset/ADL5Opportunity_taskB2_S1.csv\",header=None)\n",
    "drill_1 = pd.read_csv(\"./reduced_dataset/Drill1Opportunity_taskB2.csv\",header=None)\n",
    "adl_2_1 = pd.read_csv(\"./reduced_dataset/ADL1Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_2 = pd.read_csv(\"./reduced_dataset/ADL2Opportunity_taskB2_S2.csv\",header=None)\n",
    "drill_2 = pd.read_csv(\"./reduced_dataset/Drill2Opportunity_taskB2.csv\",header=None)\n",
    "adl_3_1 = pd.read_csv(\"./reduced_dataset/ADL1Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_2 = pd.read_csv(\"./reduced_dataset/ADL2Opportunity_taskB2_S3.csv\",header=None)\n",
    "drill_3 = pd.read_csv(\"./reduced_dataset/Drill3Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "# import validation data\n",
    "adl_2_3 = pd.read_csv(\"./reduced_dataset/ADL3Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_3 = pd.read_csv(\"./reduced_dataset/ADL3Opportunity_taskB2_S3.csv\",header=None)\n",
    "\n",
    "# import test data\n",
    "adl_2_4 = pd.read_csv(\"./reduced_dataset/ADL4Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_5 = pd.read_csv(\"./reduced_dataset/ADL5Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_4 = pd.read_csv(\"./reduced_dataset/ADL4Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_5 = pd.read_csv(\"./reduced_dataset/ADL5Opportunity_taskB2_S3.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frames = [adl_1_1,adl_1_2,adl_1_3,adl_1_4,adl_1_5,drill_1,adl_2_1,adl_2_2,drill_2,adl_3_1,adl_3_2,drill_3]\n",
    "val_frames = [adl_2_3,adl_3_3]\n",
    "test_frames = [adl_2_4,adl_2_5,adl_3_4,adl_3_5]\n",
    "train_data = pd.concat(train_frames)\n",
    "val_data = pd.concat(val_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "train_data.rename(columns ={21: 'labels'}, inplace =True)\n",
    "val_data.rename(columns ={21: 'labels'}, inplace =True)\n",
    "test_data.rename(columns ={21: 'labels'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train (497014, 22), val (60949, 22), test (118750, 22)\n"
     ]
    }
   ],
   "source": [
    "print(\"shapes: train {0}, val {1}, test {2}\".format(train_data.shape, val_data.shape, test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_train, scaled_val, scaled_test, train_labels, val_labels, test_labels = prepare_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497014, 21)\n",
      "(497014, 18)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sensors = 21\n",
    "window_size = 24\n",
    "step_size = 12\n",
    "classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segments, train_labels, val_segments, val_labels, test_segments, test_labels = segment_data(scaled_train, train_labels, scaled_val, val_labels,\n",
    "                                                                                                  scaled_test, test_labels, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input for CNN\n",
    "reshaped_train = train_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_val = val_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_test = test_segments.reshape(-1, window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "size_of_kernel = (5,1)\n",
    "kernel_strides = 1\n",
    "num_filters = 64\n",
    "num_lstm_cells = 128\n",
    "dropout_prob = 0.5\n",
    "inputshape = (window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(BatchNormalization(input_shape=inputshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='glorot_normal', name='1_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 activation='relu',kernel_initializer='glorot_normal',\n",
    "                 name='2_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 activation='relu',kernel_initializer='glorot_normal',\n",
    "                 name='3_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=(5,3), strides=(1,3),\n",
    "                 activation='relu',kernel_initializer='glorot_normal',\n",
    "                 name='2D_conv_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Reshape((8, num_filters*7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.add(CuDNNLSTM(num_lstm_cells,kernel_initializer='glorot_normal', return_sequences=True, name='1_lstm_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='2_dropout_layer'))\n",
    "\n",
    "model.add(CuDNNLSTM(num_lstm_cells,kernel_initializer='glorot_normal',return_sequences=False, name='2_lstm_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='3_dropout_layer'))\n",
    "\n",
    "model.add(Dense(int(num_lstm_cells/2),kernel_initializer='glorot_normal',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer'))\n",
    "\n",
    "model.add(Dense(classes,kernel_initializer='glorot_normal',\n",
    "                bias_initializer=initializers.Constant(value=0.1),activation='softmax', name='softmax_layer'))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_2: input shape: (None, 24, 21, 1) output shape: (None, 24, 21, 1)\n",
      "1_conv_layer: input shape: (None, 24, 21, 1) output shape: (None, 20, 21, 64)\n",
      "2_conv_layer: input shape: (None, 20, 21, 64) output shape: (None, 16, 21, 64)\n",
      "3_conv_layer: input shape: (None, 16, 21, 64) output shape: (None, 12, 21, 64)\n",
      "2D_conv_layer: input shape: (None, 12, 21, 64) output shape: (None, 8, 7, 64)\n",
      "reshape_1: input shape: (None, 8, 7, 64) output shape: (None, 8, 448)\n",
      "1_lstm_layer: input shape: (None, 8, 448) output shape: (None, 8, 128)\n",
      "2_dropout_layer: input shape: (None, 8, 128) output shape: (None, 8, 128)\n",
      "2_lstm_layer: input shape: (None, 8, 128) output shape: (None, 128)\n",
      "3_dropout_layer: input shape: (None, 128) output shape: (None, 128)\n",
      "dense_layer: input shape: (None, 128) output shape: (None, 64)\n",
      "softmax_layer: input shape: (None, 64) output shape: (None, 18)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(str(layer.name) + ': input shape: ' + str(layer.input_shape) + ' output shape: ' + str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41416 samples, validate on 5078 samples\n",
      "Epoch 1/50\n",
      "41416/41416 [==============================] - 14s 347us/step - loss: 1.0795 - acc: 0.7114 - val_loss: 0.8137 - val_acc: 0.8232\n",
      "Epoch 2/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.6719 - acc: 0.7792 - val_loss: 0.6009 - val_acc: 0.8389\n",
      "Epoch 3/50\n",
      "41416/41416 [==============================] - 10s 249us/step - loss: 0.5653 - acc: 0.8045 - val_loss: 0.5415 - val_acc: 0.8411\n",
      "Epoch 4/50\n",
      "41416/41416 [==============================] - 10s 248us/step - loss: 0.4961 - acc: 0.8249 - val_loss: 0.5256 - val_acc: 0.8501\n",
      "Epoch 5/50\n",
      "41416/41416 [==============================] - 10s 248us/step - loss: 0.4591 - acc: 0.8388 - val_loss: 0.4883 - val_acc: 0.8545\n",
      "Epoch 6/50\n",
      "41416/41416 [==============================] - 10s 249us/step - loss: 0.4319 - acc: 0.8465 - val_loss: 0.5065 - val_acc: 0.8547\n",
      "Epoch 7/50\n",
      "41416/41416 [==============================] - 10s 250us/step - loss: 0.4077 - acc: 0.8559 - val_loss: 0.4635 - val_acc: 0.8608\n",
      "Epoch 8/50\n",
      "41416/41416 [==============================] - 10s 251us/step - loss: 0.3892 - acc: 0.8631 - val_loss: 0.5059 - val_acc: 0.8671\n",
      "Epoch 9/50\n",
      "41416/41416 [==============================] - 10s 251us/step - loss: 0.3669 - acc: 0.8707 - val_loss: 0.4800 - val_acc: 0.8633\n",
      "Epoch 10/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.3512 - acc: 0.8748 - val_loss: 0.5005 - val_acc: 0.8478\n",
      "Epoch 11/50\n",
      "41416/41416 [==============================] - 11s 254us/step - loss: 0.3277 - acc: 0.8856 - val_loss: 0.4928 - val_acc: 0.8675\n",
      "Epoch 12/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.3141 - acc: 0.8894 - val_loss: 0.4855 - val_acc: 0.8673\n",
      "Epoch 13/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.3077 - acc: 0.8944 - val_loss: 0.4650 - val_acc: 0.8736\n",
      "Epoch 14/50\n",
      "41416/41416 [==============================] - 10s 250us/step - loss: 0.2949 - acc: 0.8986 - val_loss: 0.4935 - val_acc: 0.8765\n",
      "Epoch 15/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.2830 - acc: 0.9043 - val_loss: 0.4824 - val_acc: 0.8751\n",
      "Epoch 16/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.2767 - acc: 0.9048 - val_loss: 0.4763 - val_acc: 0.8669\n",
      "Epoch 17/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.2628 - acc: 0.9103 - val_loss: 0.5059 - val_acc: 0.8675\n",
      "Epoch 18/50\n",
      "41416/41416 [==============================] - 10s 251us/step - loss: 0.2633 - acc: 0.9105 - val_loss: 0.5287 - val_acc: 0.8755\n",
      "Epoch 19/50\n",
      "41416/41416 [==============================] - 10s 251us/step - loss: 0.2529 - acc: 0.9129 - val_loss: 0.5503 - val_acc: 0.8641\n",
      "Epoch 20/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.2374 - acc: 0.9169 - val_loss: 0.5578 - val_acc: 0.8582\n",
      "Epoch 21/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.2319 - acc: 0.9211 - val_loss: 0.5350 - val_acc: 0.8690\n",
      "Epoch 22/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.2209 - acc: 0.9246 - val_loss: 0.5468 - val_acc: 0.8734\n",
      "Epoch 23/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.2247 - acc: 0.9223 - val_loss: 0.5409 - val_acc: 0.8803\n",
      "Epoch 24/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.2152 - acc: 0.9262 - val_loss: 0.5768 - val_acc: 0.8738\n",
      "Epoch 25/50\n",
      "41416/41416 [==============================] - 10s 251us/step - loss: 0.2023 - acc: 0.9309 - val_loss: 0.5214 - val_acc: 0.8746\n",
      "Epoch 26/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.2062 - acc: 0.9286 - val_loss: 0.5605 - val_acc: 0.8850\n",
      "Epoch 27/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.1902 - acc: 0.9341 - val_loss: 0.5792 - val_acc: 0.8724\n",
      "Epoch 28/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.1879 - acc: 0.9356 - val_loss: 0.5580 - val_acc: 0.8714\n",
      "Epoch 29/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.1763 - acc: 0.9396 - val_loss: 0.5755 - val_acc: 0.8769\n",
      "Epoch 30/50\n",
      "41416/41416 [==============================] - 10s 250us/step - loss: 0.1767 - acc: 0.9391 - val_loss: 0.6046 - val_acc: 0.8653\n",
      "Epoch 31/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.1715 - acc: 0.9413 - val_loss: 0.6521 - val_acc: 0.8710\n",
      "Epoch 32/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.1668 - acc: 0.9424 - val_loss: 0.6121 - val_acc: 0.8765\n",
      "Epoch 33/50\n",
      "41416/41416 [==============================] - 11s 261us/step - loss: 0.1580 - acc: 0.9462 - val_loss: 0.6176 - val_acc: 0.8730\n",
      "Epoch 34/50\n",
      "41416/41416 [==============================] - 11s 260us/step - loss: 0.1615 - acc: 0.9442 - val_loss: 0.6528 - val_acc: 0.8677\n",
      "Epoch 35/50\n",
      "41416/41416 [==============================] - 10s 248us/step - loss: 0.1600 - acc: 0.9457 - val_loss: 0.6712 - val_acc: 0.8592\n",
      "Epoch 36/50\n",
      "41416/41416 [==============================] - 10s 246us/step - loss: 0.1457 - acc: 0.9492 - val_loss: 0.6131 - val_acc: 0.8698\n",
      "Epoch 37/50\n",
      "41416/41416 [==============================] - 10s 251us/step - loss: 0.1399 - acc: 0.9523 - val_loss: 0.6300 - val_acc: 0.8720\n",
      "Epoch 38/50\n",
      "41416/41416 [==============================] - 11s 274us/step - loss: 0.1430 - acc: 0.9509 - val_loss: 0.6255 - val_acc: 0.8661\n",
      "Epoch 39/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.1355 - acc: 0.9526 - val_loss: 0.6822 - val_acc: 0.8675\n",
      "Epoch 40/50\n",
      "41416/41416 [==============================] - 10s 246us/step - loss: 0.1463 - acc: 0.9500 - val_loss: 0.6761 - val_acc: 0.8665\n",
      "Epoch 41/50\n",
      "41416/41416 [==============================] - 11s 257us/step - loss: 0.1391 - acc: 0.9527 - val_loss: 0.6386 - val_acc: 0.8789\n",
      "Epoch 42/50\n",
      "41416/41416 [==============================] - 10s 250us/step - loss: 0.1302 - acc: 0.9554 - val_loss: 0.6701 - val_acc: 0.8734\n",
      "Epoch 43/50\n",
      "41416/41416 [==============================] - 10s 247us/step - loss: 0.1260 - acc: 0.9575 - val_loss: 0.6632 - val_acc: 0.8643\n",
      "Epoch 44/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.1195 - acc: 0.9593 - val_loss: 0.7123 - val_acc: 0.8683\n",
      "Epoch 45/50\n",
      "41416/41416 [==============================] - 10s 253us/step - loss: 0.1169 - acc: 0.9615 - val_loss: 0.7523 - val_acc: 0.8667\n",
      "Epoch 46/50\n",
      "41416/41416 [==============================] - 11s 258us/step - loss: 0.1141 - acc: 0.9603 - val_loss: 0.8220 - val_acc: 0.8584\n",
      "Epoch 47/50\n",
      "41416/41416 [==============================] - 11s 265us/step - loss: 0.1133 - acc: 0.9612 - val_loss: 0.7072 - val_acc: 0.8629\n",
      "Epoch 48/50\n",
      "41416/41416 [==============================] - 11s 261us/step - loss: 0.1093 - acc: 0.9634 - val_loss: 0.7106 - val_acc: 0.8661\n",
      "Epoch 49/50\n",
      "41416/41416 [==============================] - 10s 249us/step - loss: 0.1106 - acc: 0.9620 - val_loss: 0.7668 - val_acc: 0.8629\n",
      "Epoch 50/50\n",
      "41416/41416 [==============================] - 10s 252us/step - loss: 0.1103 - acc: 0.9622 - val_loss: 0.7267 - val_acc: 0.8627\n",
      "Calculating score.. \n",
      "9894/9894 [==============================] - 2s 227us/step\n",
      "[0.69259616540052305, 0.86031938548615328]\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "train_epoches = 50\n",
    "\n",
    "model.fit(reshaped_train,train_labels,validation_data=(reshaped_val,val_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(reshaped_test,test_labels,verbose=1)\n",
    "print(score)\n",
    "model.save('taskB2_all_Subjects_CNN_LSTM_our_elaboration_reduced.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_epoches = 20\n",
    "#all_train = np.concatenate((reshaped_train, reshaped_val))\n",
    "#all_labels = np.concatenate((train_labels, val_labels))\n",
    "#model.fit(all_train,all_labels,validation_data=(reshaped_test,test_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(reshaped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy is 0.8603193854861533\n",
      "0 0.832524762482 0.931251512951\n",
      "1 0.0058621386699 0.608695652174\n",
      "2 0.00960177885587 0.883720930233\n",
      "3 0.00606428138266 0.59872611465\n",
      "4 0.00838892257934 0.847457627119\n",
      "5 0.0230442692541 0.547738693467\n",
      "6 0.0161714170204 0.65671641791\n",
      "7 0.0101071356378 0.350364963504\n",
      "8 0.00778249444108 0.31884057971\n",
      "9 0.00394178289873 0.329411764706\n",
      "10 0.00424499696786 0.0338983050847\n",
      "11 0.0040428542551 0.230769230769\n",
      "12 0.00262785526582 0.0952380952381\n",
      "13 0.0067717808773 0.360248447205\n",
      "14 0.00616535273903 0.292134831461\n",
      "15 0.0100060642814 0.506666666667\n",
      "16 0.0320396199717 0.487951807229\n",
      "17 0.0106124924196 0.432989690722\n",
      "The weigths sum is 1.0\n",
      "The computed f1-score is 0.8595133036902902\n",
      "The f1-score with sklearn function is 0.85951330369029\n"
     ]
    }
   ],
   "source": [
    "# F1-score measure\n",
    "from sklearn.metrics import f1_score\n",
    "num_classes = 18\n",
    "class_predictions = []\n",
    "class_true = []\n",
    "tot_labels = 0.0\n",
    "count = 0.0\n",
    "for pair in zip(predictions, test_labels):\n",
    "    class_predictions.append(np.argmax(pair[0]))\n",
    "    class_true.append(np.argmax(pair[1]))\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    tot_labels += 1.0\n",
    "    \n",
    "print('Standard accuracy is ' + str(count/tot_labels))    \n",
    "\n",
    "unique, counts = np.unique(class_true, return_counts=True)\n",
    "counted_labels = dict(zip(unique, counts))\n",
    "f1_scores = f1_score(class_predictions, class_true, average=None)\n",
    "\n",
    "tot_f1_score = 0.0\n",
    "weights_sum = 0.0\n",
    "for i in range(num_classes):\n",
    "    labels_class_i = counted_labels[i]\n",
    "    weight_i = labels_class_i / tot_labels\n",
    "    weights_sum += weight_i\n",
    "    tot_f1_score += f1_scores[i]*weight_i\n",
    "    print(str(i) + ' ' + str(weight_i) + ' ' + str(f1_scores[i]))\n",
    "\n",
    "    \n",
    "print('The weigths sum is ' + str(weights_sum))\n",
    "print('The computed f1-score is {}'.format(tot_f1_score))\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(class_true, class_predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.to_csv('preds_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(testY)\n",
    "true_df.to_csv('true_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
