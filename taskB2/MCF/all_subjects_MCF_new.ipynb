{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have chosen to select most frequent label of the window as segment label\n",
      "Importing data...\n",
      "shapes: train (515444, 113), val (119354, 113), test (234589, 113)\n",
      "Computing features for each channel segment...\n",
      "features extracted from training data\n",
      "features extracted from val data\n",
      "features extracted from test data\n",
      "New shapes: train (42952, 1, 904), val (9945, 1, 904), test (19548, 1, 904)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import sys\n",
    "\n",
    "from scipy.stats import skew\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "is_most_freq = True\n",
    "if is_most_freq:\n",
    "    print('You have chosen to select most frequent label of the window as segment label')\n",
    "\n",
    "def slidingWindow(sequence, labels, winSize, step, noNull):\n",
    "\n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    "\n",
    "    # number of chunks\n",
    "    numOfChunks = ((len(sequence)-winSize)//step)+1\n",
    "\n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        segment = sequence[i:i+winSize]\n",
    "        seg_labels = labels[i:i+winSize]\n",
    "        if noNull:\n",
    "            if seg_labels[-1] != 0:\n",
    "                yield segment, seg_labels\n",
    "        else:\n",
    "            yield segment, seg_labels\n",
    "\n",
    "def get_most_frequent(labels):\n",
    "\n",
    "    (values, counts) = np.unique(labels, return_counts=True)\n",
    "    index = np.argmax(counts)\n",
    "    return values[index]\n",
    "\n",
    "def get_features_from_segment(segments):\n",
    "    num_features = 8\n",
    "    features = []\n",
    "    for i in range(segments.shape[1]):\n",
    "        segment = segments[:,i]\n",
    "\n",
    "        # features for each segment\n",
    "        maxi = np.amax(segment)\n",
    "        mini = np.amin(segment)\n",
    "        avg = np.mean(segment)\n",
    "        stdev = np.std(segment)\n",
    "        vari = stdev**2\n",
    "        mediano = np.median(segment)\n",
    "        skewness = skew(segment)\n",
    "        autocorr = np.correlate(segment, segment)\n",
    "        #abs_fft = np.abs(np.fft.fft(segment))\n",
    "        #idx_max_fft = np.argmax(abs_fft)\n",
    "        #max_fft = np.max(abs_fft)\n",
    "        features.append([maxi, mini, avg, stdev, vari, mediano, skewness, autocorr])\n",
    "\n",
    "    return np.array(features).reshape(-1, num_features*segments.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def segment_data(X_train, y_train, X_val, y_val, X_test, y_test, winSize, step, noNull=False):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    assert len(X_test) == len(y_test)\n",
    "    # obtain chunks of data\n",
    "    train_chunks = slidingWindow(X_train, y_train , winSize, step, noNull)\n",
    "    val_chunks = slidingWindow(X_val, y_val, winSize, step, noNull)\n",
    "    test_chunks = slidingWindow(X_test, y_test, winSize, step, noNull)\n",
    "\n",
    "    # segment the data\n",
    "    train_segments = []\n",
    "    train_labels = []\n",
    "    for chunk in train_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        train_segments.append(get_features_from_segment(data))\n",
    "        if is_most_freq:\n",
    "            train_labels.append(get_most_frequent(labels))\n",
    "        else:\n",
    "            train_labels.append(labels[-1])\n",
    "            \n",
    "    print('features extracted from training data')\n",
    "\n",
    "    val_segments = []\n",
    "    val_labels = []\n",
    "    for chunk in val_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        val_segments.append(get_features_from_segment(data))\n",
    "        if is_most_freq:\n",
    "            val_labels.append(get_most_frequent(labels))\n",
    "        else:\n",
    "            val_labels.append(labels[-1])\n",
    "    \n",
    "    print('features extracted from val data')\n",
    "    \n",
    "    test_segments = []\n",
    "    test_labels = []\n",
    "    for chunk in test_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        test_segments.append(get_features_from_segment(data))\n",
    "        if is_most_freq:\n",
    "            test_labels.append(get_most_frequent(labels))\n",
    "        else:\n",
    "            test_labels.append(labels[-1])\n",
    "            \n",
    "    print('features extracted from test data')\n",
    "\n",
    "    return np.array(train_segments), np.array(train_labels), np.array(val_segments), np.array(val_labels), np.array(test_segments), np.array(test_labels)\n",
    "\n",
    "def prepare_data(train_data, val_data, test_data):\n",
    "\n",
    "    train_labels = train_data['labels'].values\n",
    "    val_labels = val_data['labels'].values\n",
    "    test_labels = test_data['labels'].values\n",
    "\n",
    "    train_data.drop(['labels'], axis=1, inplace=True)\n",
    "    val_data.drop(['labels'], axis=1, inplace=True)\n",
    "    test_data.drop(['labels'], axis=1, inplace=True)\n",
    "\n",
    "    return train_data.values, val_data.values, test_data.values, train_labels, val_labels, test_labels\n",
    "\n",
    "print('Importing data...')\n",
    "# import train data\n",
    "adl_1_1 = pd.read_csv(\"../full_dataset/ADL1Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_2 = pd.read_csv(\"../full_dataset/ADL2Opportunity_taskB2_S1.csv\",header=None)\n",
    "drill_1 = pd.read_csv(\"../full_dataset/Drill1Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "adl_2_1 = pd.read_csv(\"../full_dataset/ADL1Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_2 = pd.read_csv(\"../full_dataset/ADL2Opportunity_taskB2_S2.csv\",header=None)\n",
    "drill_2 = pd.read_csv(\"../full_dataset/Drill2Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "adl_3_1 = pd.read_csv(\"../full_dataset/ADL1Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_2 = pd.read_csv(\"../full_dataset/ADL2Opportunity_taskB2_S3.csv\",header=None)\n",
    "drill_3 = pd.read_csv(\"../full_dataset/Drill3Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "adl_4_1 = pd.read_csv(\"../full_dataset/ADL1Opportunity_taskB2_S4.csv\",header=None)\n",
    "adl_4_2 = pd.read_csv(\"../full_dataset/ADL2Opportunity_taskB2_S4.csv\",header=None)\n",
    "drill_4 = pd.read_csv(\"../full_dataset/Drill4Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "# import validation data\n",
    "adl_1_3 = pd.read_csv(\"../full_dataset/ADL3Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_2_3 = pd.read_csv(\"../full_dataset/ADL3Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_3 = pd.read_csv(\"../full_dataset/ADL3Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_4_3 = pd.read_csv(\"../full_dataset/ADL3Opportunity_taskB2_S4.csv\",header=None)\n",
    "\n",
    "# import test data\n",
    "adl_1_4 = pd.read_csv(\"../full_dataset/ADL4Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_5 = pd.read_csv(\"../full_dataset/ADL5Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_2_4 = pd.read_csv(\"../full_dataset/ADL4Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_5 = pd.read_csv(\"../full_dataset/ADL5Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_4 = pd.read_csv(\"../full_dataset/ADL4Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_5 = pd.read_csv(\"../full_dataset/ADL5Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_4_4 = pd.read_csv(\"../full_dataset/ADL4Opportunity_taskB2_S4.csv\",header=None)\n",
    "adl_4_5 = pd.read_csv(\"../full_dataset/ADL5Opportunity_taskB2_S4.csv\",header=None)\n",
    "\n",
    "train_frames = [adl_1_1, adl_1_2, drill_1, adl_2_1, adl_2_2, drill_2, adl_3_1, adl_3_2, drill_3, adl_4_1, adl_4_2, drill_4]\n",
    "val_frames = [adl_1_3, adl_2_3, adl_3_3, adl_4_3]\n",
    "test_frames = [adl_1_4, adl_1_5, adl_2_4, adl_2_5, adl_3_4, adl_3_5, adl_4_4, adl_4_5]\n",
    "train_data = pd.concat(train_frames)\n",
    "val_data = pd.concat(val_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "train_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "val_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "test_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "\n",
    "train_data, val_data, test_data, train_labels, val_labels, test_labels = prepare_data(train_data, val_data, test_data)\n",
    "\n",
    "num_sensors = 113\n",
    "window_size = 24\n",
    "step_size = 12\n",
    "classes = 18\n",
    "print(\"shapes: train {0}, val {1}, test {2}\".format(train_data.shape, val_data.shape, test_data.shape))\n",
    "# compute features for each window\n",
    "print('Computing features for each channel segment...')\n",
    "train_segments, train_labels, val_segments, val_labels, test_segments, test_labels = segment_data(train_data, train_labels, val_data, val_labels,\n",
    "                                                                                                  test_data, test_labels, window_size, step_size)\n",
    "\n",
    "print(\"New shapes: train {0}, val {1}, test {2}\".format(train_segments.shape, val_segments.shape, test_segments.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA components: 353\n",
      "Test accuracy is: 0.8903212604870063\n",
      "The f1-score with sklearn function is 0.8807866151141867\n",
      "Average f1-score is 0.547960261440675\n"
     ]
    }
   ],
   "source": [
    "num_features = 8\n",
    "new_train = train_segments.reshape(-1, num_sensors*num_features)\n",
    "\n",
    "new_val = val_segments.reshape(-1, num_sensors*num_features)\n",
    "new_test = test_segments.reshape(-1, num_sensors*num_features)\n",
    "\n",
    "all_train = np.concatenate([new_train, new_val])\n",
    "\n",
    "all_labels = np.concatenate([train_labels, val_labels])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_train = scaler.fit_transform(all_train)\n",
    "scaled_test = scaler.transform(new_test)\n",
    "\n",
    "pca = PCA(0.99)\n",
    "pca.fit(scaled_train)\n",
    "\n",
    "print('PCA components: {}'.format(pca.n_components_))\n",
    "\n",
    "pca_train = pca.transform(scaled_train)\n",
    "pca_test = pca.transform(scaled_test)\n",
    "\n",
    "# train a one-vs-one multi-class support vector machine\n",
    "clf = SVC(decision_function_shape='ovr', kernel='linear')\n",
    "clf.fit(pca_train, all_labels)\n",
    "\n",
    "# predict test data\n",
    "svm_pred = clf.predict(pca_test)\n",
    "\n",
    "# measure accuracy and f1-score\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "\n",
    "for pair in zip(svm_pred, test_labels):\n",
    "    if pair[0] == pair[1]:\n",
    "        num += 1.0\n",
    "    \n",
    "    den += 1.0\n",
    "    \n",
    "print('Test accuracy is: {}'.format(num / den))\n",
    "f1_scores = f1_score(svm_pred, test_labels, average=None)\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(test_labels, svm_pred, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
