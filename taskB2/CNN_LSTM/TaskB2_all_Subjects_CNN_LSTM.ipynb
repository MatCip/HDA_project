{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, LSTM, CuDNNLSTM, Flatten, Dropout, Reshape, BatchNormalization, PReLU, ELU\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import keras as ke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindow(sequence, labels, winSize, step, noNull):\n",
    "\n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # number of chunks\n",
    "    numOfChunks = ((len(sequence)-winSize)//step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        segment = sequence[i:i+winSize]\n",
    "        seg_labels = labels[i:i+winSize]\n",
    "        if noNull:\n",
    "            if seg_labels[-1] != 0:\n",
    "                yield segment, seg_labels\n",
    "        else:\n",
    "            yield segment, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_data(X_train, y_train, X_val, y_val, X_test, y_test, winSize, step, noNull=False):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    assert len(X_test) == len(y_test)\n",
    "    # obtain chunks of data\n",
    "    train_chunks = slidingWindow(X_train, y_train , winSize, step, noNull)\n",
    "    val_chunks = slidingWindow(X_val, y_val, winSize, step, noNull)\n",
    "    test_chunks = slidingWindow(X_test, y_test, winSize, step, noNull)\n",
    "    \n",
    "    # segment the data\n",
    "    train_segments = []\n",
    "    train_labels = []\n",
    "    for chunk in train_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        train_segments.append(data)\n",
    "        train_labels.append(labels[-1])\n",
    "        \n",
    "    val_segments = []\n",
    "    val_labels = []\n",
    "    for chunk in val_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        val_segments.append(data)\n",
    "        val_labels.append(labels[-1])\n",
    "    \n",
    "    test_segments = []\n",
    "    test_labels = []\n",
    "    for chunk in test_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        test_segments.append(data)\n",
    "        test_labels.append(labels[-1])\n",
    "        \n",
    "    return np.array(train_segments), np.array(train_labels), np.array(val_segments), np.array(val_labels), np.array(test_segments), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, test_data):\n",
    "    encoder = OneHotEncoder()\n",
    "    train_labels = encoder.fit_transform(train_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    val_labels = encoder.transform(val_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    test_labels = encoder.transform(test_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data.drop(['labels'], axis=1, inplace=True)\n",
    "    val_data.drop(['labels'], axis=1, inplace=True)\n",
    "    test_data.drop(['labels'], axis=1, inplace=True)\n",
    "    data = pd.concat([train_data,val_data,test_data])\n",
    "    scaler.fit(data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import train data\n",
    "adl_1_1 = pd.read_csv(\"../full_dataset/ADL1Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_2 = pd.read_csv(\"../full_dataset/ADL2Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_3 = pd.read_csv(\"../full_dataset/ADL3Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_4 = pd.read_csv(\"../full_dataset/ADL4Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_5 = pd.read_csv(\"../full_dataset/ADL5Opportunity_taskB2_S1.csv\",header=None)\n",
    "drill_1 = pd.read_csv(\"../full_dataset/Drill1Opportunity_taskB2.csv\",header=None)\n",
    "adl_2_1 = pd.read_csv(\"../full_dataset/ADL1Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_2 = pd.read_csv(\"../full_dataset/ADL2Opportunity_taskB2_S2.csv\",header=None)\n",
    "drill_2 = pd.read_csv(\"../full_dataset/Drill2Opportunity_taskB2.csv\",header=None)\n",
    "adl_3_1 = pd.read_csv(\"../full_dataset/ADL1Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_2 = pd.read_csv(\"../full_dataset/ADL2Opportunity_taskB2_S3.csv\",header=None)\n",
    "drill_3 = pd.read_csv(\"../full_dataset/Drill3Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "# import validation data\n",
    "adl_2_3 = pd.read_csv(\"../full_dataset/ADL3Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_3 = pd.read_csv(\"../full_dataset/ADL3Opportunity_taskB2_S3.csv\",header=None)\n",
    "\n",
    "# import test data\n",
    "adl_2_4 = pd.read_csv(\"../full_dataset/ADL4Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_5 = pd.read_csv(\"../full_dataset/ADL5Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_4 = pd.read_csv(\"../full_dataset/ADL4Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_5 = pd.read_csv(\"../full_dataset/ADL5Opportunity_taskB2_S3.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frames = [adl_1_1,adl_1_2,adl_1_3,adl_1_4,adl_1_5,drill_1,adl_2_1,adl_2_2,drill_2,adl_3_1,adl_3_2,drill_3]\n",
    "val_frames = [adl_2_3,adl_3_3]\n",
    "test_frames = [adl_2_4,adl_2_5,adl_3_4,adl_3_5]\n",
    "train_data = pd.concat(train_frames)\n",
    "val_data = pd.concat(val_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "train_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "val_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "test_data.rename(columns ={113: 'labels'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train (497014, 114), val (60949, 114), test (118750, 114)\n"
     ]
    }
   ],
   "source": [
    "print(\"shapes: train {0}, val {1}, test {2}\".format(train_data.shape, val_data.shape, test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_train, scaled_val, scaled_test, train_labels, val_labels, test_labels = prepare_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497014, 113)\n",
      "(497014, 18)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sensors = 113\n",
    "window_size = 24\n",
    "step_size = 12\n",
    "classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segments, train_labels, val_segments, val_labels, test_segments, test_labels = segment_data(scaled_train, train_labels, scaled_val, val_labels,\n",
    "                                                                                                  scaled_test, test_labels, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input for CNN\n",
    "reshaped_train = train_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_val = val_segments.reshape(-1, window_size, num_sensors, 1)\n",
    "reshaped_test = test_segments.reshape(-1, window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "size_of_kernel = (5,1)\n",
    "kernel_strides = 1\n",
    "num_filters = 64\n",
    "num_lstm_cells = 128\n",
    "dropout_prob = 0.5\n",
    "inputshape = (window_size, num_sensors, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(BatchNormalization(input_shape=inputshape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 kernel_initializer='glorot_normal', name='1_conv_layer'))\n",
    "model.add(PReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 name='2_conv_layer'))\n",
    "model.add(PReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 name='3_conv_layer'))\n",
    "model.add(PReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(num_filters, kernel_size=size_of_kernel, strides=kernel_strides,\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 name='4_conv_layer'))\n",
    "model.add(PReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Reshape((8, num_filters*num_sensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.add(CuDNNLSTM(num_lstm_cells,kernel_initializer='glorot_normal', return_sequences=True, name='1_lstm_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='2_dropout_layer'))\n",
    "\n",
    "model.add(CuDNNLSTM(num_lstm_cells,kernel_initializer='glorot_normal',return_sequences=False, name='2_lstm_layer'))\n",
    "\n",
    "model.add(Dropout(dropout_prob, name='3_dropout_layer'))\n",
    "\n",
    "model.add(Dense(64,kernel_initializer='glorot_normal',\n",
    "                bias_initializer=initializers.Constant(value=0.1), name='dense_layer'))\n",
    "model.add(Dropout(dropout_prob, name='4_dropout_layer'))\n",
    "\n",
    "model.add(Dense(classes,kernel_initializer='glorot_normal',\n",
    "                bias_initializer=initializers.Constant(value=0.1),activation='softmax', name='softmax_layer'))\n",
    "\n",
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_1: input shape: (None, 24, 113, 1) output shape: (None, 24, 113, 1)\n",
      "1_conv_layer: input shape: (None, 24, 113, 1) output shape: (None, 20, 113, 64)\n",
      "p_re_lu_1: input shape: (None, 20, 113, 64) output shape: (None, 20, 113, 64)\n",
      "2_conv_layer: input shape: (None, 20, 113, 64) output shape: (None, 16, 113, 64)\n",
      "p_re_lu_2: input shape: (None, 16, 113, 64) output shape: (None, 16, 113, 64)\n",
      "3_conv_layer: input shape: (None, 16, 113, 64) output shape: (None, 12, 113, 64)\n",
      "p_re_lu_3: input shape: (None, 12, 113, 64) output shape: (None, 12, 113, 64)\n",
      "4_conv_layer: input shape: (None, 12, 113, 64) output shape: (None, 8, 113, 64)\n",
      "p_re_lu_4: input shape: (None, 8, 113, 64) output shape: (None, 8, 113, 64)\n",
      "reshape_1: input shape: (None, 8, 113, 64) output shape: (None, 8, 7232)\n",
      "1_lstm_layer: input shape: (None, 8, 7232) output shape: (None, 8, 128)\n",
      "2_dropout_layer: input shape: (None, 8, 128) output shape: (None, 8, 128)\n",
      "2_lstm_layer: input shape: (None, 8, 128) output shape: (None, 128)\n",
      "3_dropout_layer: input shape: (None, 128) output shape: (None, 128)\n",
      "dense_layer: input shape: (None, 128) output shape: (None, 64)\n",
      "4_dropout_layer: input shape: (None, 64) output shape: (None, 64)\n",
      "softmax_layer: input shape: (None, 64) output shape: (None, 18)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(str(layer.name) + ': input shape: ' + str(layer.input_shape) + ' output shape: ' + str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41416 samples, validate on 5078 samples\n",
      "Epoch 1/50\n",
      "41416/41416 [==============================] - 70s 2ms/step - loss: 1.1169 - acc: 0.7113 - val_loss: 0.6594 - val_acc: 0.8358\n",
      "Epoch 2/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.7242 - acc: 0.7747 - val_loss: 0.5463 - val_acc: 0.8387\n",
      "Epoch 3/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.5851 - acc: 0.7989 - val_loss: 0.5238 - val_acc: 0.8513\n",
      "Epoch 4/50\n",
      "41416/41416 [==============================] - 65s 2ms/step - loss: 0.4985 - acc: 0.8214 - val_loss: 0.4790 - val_acc: 0.8584\n",
      "Epoch 5/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.4499 - acc: 0.8339 - val_loss: 0.4800 - val_acc: 0.8578\n",
      "Epoch 6/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.4003 - acc: 0.8463 - val_loss: 0.4748 - val_acc: 0.8673\n",
      "Epoch 7/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.3665 - acc: 0.8595 - val_loss: 0.4605 - val_acc: 0.8651\n",
      "Epoch 8/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.3455 - acc: 0.8713 - val_loss: 0.4422 - val_acc: 0.8692\n",
      "Epoch 9/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.3058 - acc: 0.8842 - val_loss: 0.5459 - val_acc: 0.8714\n",
      "Epoch 10/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.2959 - acc: 0.8902 - val_loss: 0.5286 - val_acc: 0.8633\n",
      "Epoch 11/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.2755 - acc: 0.8960 - val_loss: 0.5375 - val_acc: 0.8718\n",
      "Epoch 12/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.2562 - acc: 0.9031 - val_loss: 0.5150 - val_acc: 0.8724\n",
      "Epoch 13/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.2469 - acc: 0.9084 - val_loss: 0.5623 - val_acc: 0.8824\n",
      "Epoch 14/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.2299 - acc: 0.9164 - val_loss: 0.5487 - val_acc: 0.8854\n",
      "Epoch 15/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.2203 - acc: 0.9208 - val_loss: 0.5389 - val_acc: 0.8842\n",
      "Epoch 16/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.2057 - acc: 0.9293 - val_loss: 0.5252 - val_acc: 0.8809\n",
      "Epoch 17/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.1909 - acc: 0.9354 - val_loss: 0.5718 - val_acc: 0.8866\n",
      "Epoch 18/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.1745 - acc: 0.9431 - val_loss: 0.6049 - val_acc: 0.8836\n",
      "Epoch 19/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.1648 - acc: 0.9464 - val_loss: 0.5896 - val_acc: 0.8883\n",
      "Epoch 20/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.1491 - acc: 0.9525 - val_loss: 0.6663 - val_acc: 0.8856\n",
      "Epoch 21/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.1358 - acc: 0.9576 - val_loss: 0.7205 - val_acc: 0.8803\n",
      "Epoch 22/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.1313 - acc: 0.9578 - val_loss: 0.6476 - val_acc: 0.8832\n",
      "Epoch 23/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.1218 - acc: 0.9628 - val_loss: 0.7111 - val_acc: 0.8891\n",
      "Epoch 24/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.1137 - acc: 0.9641 - val_loss: 0.7078 - val_acc: 0.8887\n",
      "Epoch 25/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.1133 - acc: 0.9643 - val_loss: 0.7712 - val_acc: 0.8915\n",
      "Epoch 26/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.1085 - acc: 0.9667 - val_loss: 0.7185 - val_acc: 0.8836\n",
      "Epoch 27/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.0943 - acc: 0.9709 - val_loss: 0.7208 - val_acc: 0.8889\n",
      "Epoch 28/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.1004 - acc: 0.9684 - val_loss: 0.7230 - val_acc: 0.8876\n",
      "Epoch 29/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0896 - acc: 0.9726 - val_loss: 0.6550 - val_acc: 0.8911\n",
      "Epoch 30/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0804 - acc: 0.9754 - val_loss: 0.6876 - val_acc: 0.8915\n",
      "Epoch 31/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0767 - acc: 0.9766 - val_loss: 0.7522 - val_acc: 0.8960\n",
      "Epoch 32/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0760 - acc: 0.9766 - val_loss: 0.7930 - val_acc: 0.8901\n",
      "Epoch 33/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0674 - acc: 0.9799 - val_loss: 0.6497 - val_acc: 0.8982\n",
      "Epoch 34/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0655 - acc: 0.9807 - val_loss: 0.7916 - val_acc: 0.8927\n",
      "Epoch 35/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.0538 - acc: 0.9837 - val_loss: 0.8775 - val_acc: 0.8950\n",
      "Epoch 36/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0653 - acc: 0.9801 - val_loss: 0.8292 - val_acc: 0.8919\n",
      "Epoch 37/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0547 - acc: 0.9829 - val_loss: 0.8426 - val_acc: 0.8956\n",
      "Epoch 38/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0506 - acc: 0.9848 - val_loss: 0.8848 - val_acc: 0.8899\n",
      "Epoch 39/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0524 - acc: 0.9844 - val_loss: 0.8755 - val_acc: 0.8931\n",
      "Epoch 40/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.0540 - acc: 0.9839 - val_loss: 0.8390 - val_acc: 0.8911\n",
      "Epoch 41/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.0528 - acc: 0.9842 - val_loss: 0.8381 - val_acc: 0.8921\n",
      "Epoch 42/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0502 - acc: 0.9846 - val_loss: 0.7589 - val_acc: 0.8929\n",
      "Epoch 43/50\n",
      "41416/41416 [==============================] - 64s 2ms/step - loss: 0.0648 - acc: 0.9812 - val_loss: 0.7662 - val_acc: 0.8980\n",
      "Epoch 44/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0438 - acc: 0.9865 - val_loss: 0.7870 - val_acc: 0.8866\n",
      "Epoch 45/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0491 - acc: 0.9850 - val_loss: 0.7839 - val_acc: 0.8972\n",
      "Epoch 46/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0416 - acc: 0.9874 - val_loss: 0.8127 - val_acc: 0.8911\n",
      "Epoch 47/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0383 - acc: 0.9883 - val_loss: 0.8851 - val_acc: 0.9000\n",
      "Epoch 48/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0447 - acc: 0.9876 - val_loss: 0.8859 - val_acc: 0.8915\n",
      "Epoch 49/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0435 - acc: 0.9875 - val_loss: 0.9095 - val_acc: 0.8948\n",
      "Epoch 50/50\n",
      "41416/41416 [==============================] - 63s 2ms/step - loss: 0.0480 - acc: 0.9854 - val_loss: 0.8204 - val_acc: 0.8911\n",
      "Calculating score.. \n",
      "9894/9894 [==============================] - 7s 688us/step\n",
      "[0.774379607846994, 0.88447543966040021]\n",
      "Train on 46494 samples, validate on 9894 samples\n",
      "Epoch 1/20\n",
      "46494/46494 [==============================] - 75s 2ms/step - loss: 0.0979 - acc: 0.9764 - val_loss: 0.4462 - val_acc: 0.9019\n",
      "Epoch 2/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0682 - acc: 0.9823 - val_loss: 0.4326 - val_acc: 0.9083\n",
      "Epoch 3/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0595 - acc: 0.9841 - val_loss: 0.4557 - val_acc: 0.9051\n",
      "Epoch 4/20\n",
      "46494/46494 [==============================] - 76s 2ms/step - loss: 0.0524 - acc: 0.9857 - val_loss: 0.4679 - val_acc: 0.9112\n",
      "Epoch 5/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0441 - acc: 0.9876 - val_loss: 0.5080 - val_acc: 0.9051\n",
      "Epoch 6/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0460 - acc: 0.9869 - val_loss: 0.5612 - val_acc: 0.9081\n",
      "Epoch 7/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0441 - acc: 0.9876 - val_loss: 0.5260 - val_acc: 0.9094\n",
      "Epoch 8/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0511 - acc: 0.9855 - val_loss: 0.5673 - val_acc: 0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0368 - acc: 0.9894 - val_loss: 0.6037 - val_acc: 0.9097\n",
      "Epoch 10/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0378 - acc: 0.9892 - val_loss: 0.5483 - val_acc: 0.9086\n",
      "Epoch 11/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0421 - acc: 0.9873 - val_loss: 0.5597 - val_acc: 0.9094\n",
      "Epoch 12/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0347 - acc: 0.9898 - val_loss: 0.6034 - val_acc: 0.9106\n",
      "Epoch 13/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0341 - acc: 0.9902 - val_loss: 0.5999 - val_acc: 0.9095\n",
      "Epoch 14/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0339 - acc: 0.9901 - val_loss: 0.6719 - val_acc: 0.9087\n",
      "Epoch 15/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0352 - acc: 0.9900 - val_loss: 0.5919 - val_acc: 0.9081\n",
      "Epoch 16/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0331 - acc: 0.9899 - val_loss: 0.5976 - val_acc: 0.9065\n",
      "Epoch 17/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0244 - acc: 0.9932 - val_loss: 0.6563 - val_acc: 0.9105\n",
      "Epoch 18/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0319 - acc: 0.9913 - val_loss: 0.6820 - val_acc: 0.8962\n",
      "Epoch 19/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0278 - acc: 0.9919 - val_loss: 0.6986 - val_acc: 0.8982\n",
      "Epoch 20/20\n",
      "46494/46494 [==============================] - 74s 2ms/step - loss: 0.0275 - acc: 0.9926 - val_loss: 0.5947 - val_acc: 0.9099\n",
      "Calculating score.. \n",
      "9894/9894 [==============================] - 7s 678us/step\n",
      "[0.59472042971255423, 0.90994542146755608]\n"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "train_epoches = 50\n",
    "\n",
    "model.fit(reshaped_train,train_labels,validation_data=(reshaped_val,val_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(reshaped_test,test_labels,verbose=1)\n",
    "print(score)\n",
    "\n",
    "train_epoches = 20\n",
    "all_train = np.concatenate((reshaped_train, reshaped_val))\n",
    "all_labels = np.concatenate((train_labels, val_labels))\n",
    "model.fit(all_train,all_labels,validation_data=(reshaped_test,test_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(reshaped_test,test_labels,verbose=1)\n",
    "print(score)\n",
    "model.save('taskB2_all_Subjects_CNN_LSTM_our_elaboration_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1238: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Tommy Azzino\\Anaconda3\\envs\\tfdeeplearning\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = load_model('../taskB2_all_Subjects_CNN_LSTM_our_elaboration_full.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class My_History(ke.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1_scores = []\n",
    "        self.test_acc = []\n",
    "        self.f1_scores_avg = []\n",
    "        self.f1_scores_epoch = []\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_accuracy = logs.get('val_acc')\n",
    "        print('Test accuracy is {}'.format(val_accuracy))\n",
    "        self.test_acc.append(val_accuracy)\n",
    "        class_predictions = []\n",
    "        class_true = []\n",
    "        predictions = self.model.predict(self.validation_data[0])\n",
    "        for pair in zip(predictions, self.validation_data[1]):\n",
    "            class_predictions.append(np.argmax(pair[0]))\n",
    "            class_true.append(np.argmax(pair[1]))\n",
    "\n",
    "        f1_scores_i = f1_score(class_predictions, class_true, average=None)\n",
    "        self.f1_scores_epoch.append(f1_scores_i)\n",
    "        self.f1_scores_avg.append(np.mean(f1_scores_i))\n",
    "        self.f1_scores.append(f1_score(class_true, class_predictions, average='weighted'))\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning model again adding validation data...\n",
      "Train on 46494 samples, validate on 9894 samples\n",
      "Epoch 1/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9914Test accuracy is 0.9135839895742311\n",
      "Epoch 00001: val_acc improved from -inf to 0.91358, saving model to best_taskB2_all_Subjects_CNN_LSTM.h5\n",
      "46494/46494 [==============================] - 82s 2ms/step - loss: 0.0302 - acc: 0.9914 - val_loss: 0.5959 - val_acc: 0.9136\n",
      "Epoch 2/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0242 - acc: 0.9930Test accuracy is 0.908227211589979\n",
      "Epoch 00002: val_acc did not improve\n",
      "46494/46494 [==============================] - 81s 2ms/step - loss: 0.0242 - acc: 0.9930 - val_loss: 0.6525 - val_acc: 0.9082\n",
      "Epoch 3/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9930Test accuracy is 0.9055993528059572\n",
      "Epoch 00003: val_acc did not improve\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0238 - acc: 0.9930 - val_loss: 0.6792 - val_acc: 0.9056\n",
      "Epoch 4/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0230 - acc: 0.9940Test accuracy is 0.9147968471038214\n",
      "Epoch 00004: val_acc improved from 0.91358 to 0.91480, saving model to best_taskB2_all_Subjects_CNN_LSTM.h5\n",
      "46494/46494 [==============================] - 81s 2ms/step - loss: 0.0230 - acc: 0.9940 - val_loss: 0.6419 - val_acc: 0.9148\n",
      "Epoch 5/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9939Test accuracy is 0.9024661449511797\n",
      "Epoch 00005: val_acc did not improve\n",
      "46494/46494 [==============================] - 82s 2ms/step - loss: 0.0226 - acc: 0.9939 - val_loss: 0.6809 - val_acc: 0.9025\n",
      "Epoch 6/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9942Test accuracy is 0.9079239954002848\n",
      "Epoch 00006: val_acc did not improve\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0205 - acc: 0.9942 - val_loss: 0.6348 - val_acc: 0.9079\n",
      "Epoch 7/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0240 - acc: 0.9929Test accuracy is 0.9001414999471232\n",
      "Epoch 00007: val_acc did not improve\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0241 - acc: 0.9929 - val_loss: 0.7696 - val_acc: 0.9001\n",
      "Epoch 8/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9925Test accuracy is 0.9068122103355474\n",
      "Epoch 00008: val_acc did not improve\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0276 - acc: 0.9926 - val_loss: 0.6042 - val_acc: 0.9068\n",
      "Epoch 9/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0222 - acc: 0.9937Test accuracy is 0.9054982815459687\n",
      "Epoch 00009: val_acc did not improve\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0222 - acc: 0.9937 - val_loss: 0.6854 - val_acc: 0.9055\n",
      "Epoch 10/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0238 - acc: 0.9932\n",
      "Epoch 00010: reducing learning rate to 0.00010000000474974513.\n",
      "Test accuracy is 0.9141904183390263\n",
      "Epoch 00010: val_acc did not improve\n",
      "46494/46494 [==============================] - 82s 2ms/step - loss: 0.0238 - acc: 0.9932 - val_loss: 0.5880 - val_acc: 0.9142\n",
      "Epoch 11/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0110 - acc: 0.9968Test accuracy is 0.9164139896733654\n",
      "Epoch 00011: val_acc improved from 0.91480 to 0.91641, saving model to best_taskB2_all_Subjects_CNN_LSTM.h5\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0109 - acc: 0.9968 - val_loss: 0.6088 - val_acc: 0.9164\n",
      "Epoch 12/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9978Test accuracy is 0.9160097034285472\n",
      "Epoch 00012: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.6316 - val_acc: 0.9160\n",
      "Epoch 13/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0074 - acc: 0.9978Test accuracy is 0.9164139896733654\n",
      "Epoch 00013: val_acc did not improve\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.6550 - val_acc: 0.9164\n",
      "Epoch 14/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9989Test accuracy is 0.9161107770982646\n",
      "Epoch 00014: val_acc did not improve\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0049 - acc: 0.9989 - val_loss: 0.6796 - val_acc: 0.9161\n",
      "Epoch 15/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987Test accuracy is 0.917222559753273\n",
      "Epoch 00015: val_acc improved from 0.91641 to 0.91722, saving model to best_taskB2_all_Subjects_CNN_LSTM.h5\n",
      "46494/46494 [==============================] - 80s 2ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.6760 - val_acc: 0.9172\n",
      "Epoch 16/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991Test accuracy is 0.9166161309884778\n",
      "Epoch 00016: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.6880 - val_acc: 0.9166\n",
      "Epoch 17/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9989Test accuracy is 0.9173236334229904\n",
      "Epoch 00017: val_acc improved from 0.91722 to 0.91732, saving model to best_taskB2_all_Subjects_CNN_LSTM.h5\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.7080 - val_acc: 0.9173\n",
      "Epoch 18/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.9989Test accuracy is 0.9178289873132037\n",
      "Epoch 00018: val_acc improved from 0.91732 to 0.91783, saving model to best_taskB2_all_Subjects_CNN_LSTM.h5\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.7314 - val_acc: 0.9178\n",
      "Epoch 19/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0033 - acc: 0.9991Test accuracy is 0.9166161321933424\n",
      "Epoch 00019: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0033 - acc: 0.9991 - val_loss: 0.7438 - val_acc: 0.9166\n",
      "Epoch 20/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0032 - acc: 0.9991Test accuracy is 0.9161107722788068\n",
      "Epoch 00020: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.7484 - val_acc: 0.9161\n",
      "Epoch 21/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9991Test accuracy is 0.915706487238853\n",
      "Epoch 00021: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0029 - acc: 0.9991 - val_loss: 0.7658 - val_acc: 0.9157\n",
      "Epoch 22/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9991Test accuracy is 0.9175257759429672\n",
      "Epoch 00022: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.7563 - val_acc: 0.9175\n",
      "Epoch 23/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9993Test accuracy is 0.9169193471781721\n",
      "Epoch 00023: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0030 - acc: 0.9993 - val_loss: 0.7571 - val_acc: 0.9169\n",
      "Epoch 24/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0031 - acc: 0.9990\n",
      "Epoch 00024: reducing learning rate to 1.0000000474974514e-05.\n",
      "Test accuracy is 0.9171214884932846\n",
      "Epoch 00024: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.7588 - val_acc: 0.9171\n",
      "Epoch 25/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9993Test accuracy is 0.9175257735332384\n",
      "Epoch 00025: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.7565 - val_acc: 0.9175\n",
      "Epoch 26/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9994Test accuracy is 0.9173236310132614\n",
      "Epoch 00026: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.7603 - val_acc: 0.9173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0025 - acc: 0.9994Test accuracy is 0.9176268459980912\n",
      "Epoch 00027: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.7623 - val_acc: 0.9176\n",
      "Epoch 28/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0023 - acc: 0.9995Test accuracy is 0.9178289885180682\n",
      "Epoch 00028: val_acc improved from 0.91783 to 0.91783, saving model to best_taskB2_all_Subjects_CNN_LSTM.h5\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0023 - acc: 0.9995 - val_loss: 0.7649 - val_acc: 0.9178\n",
      "Epoch 29/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 00029: reducing learning rate to 1.0000000656873453e-06.\n",
      "Test accuracy is 0.9175257735332384\n",
      "Epoch 00029: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0025 - acc: 0.9994 - val_loss: 0.7657 - val_acc: 0.9175\n",
      "Epoch 30/30\n",
      "46400/46494 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994Test accuracy is 0.9175257735332384\n",
      "Epoch 00030: val_acc did not improve\n",
      "46494/46494 [==============================] - 79s 2ms/step - loss: 0.0024 - acc: 0.9994 - val_loss: 0.7661 - val_acc: 0.9175\n",
      "After other 30 epochs BEST test accuracy is: 0.9178289885180682, and BEST f1-score is 0.915783370240369\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-07bb81c44fa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# saving variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mopen_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'results_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Saving results to: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "print('Traning model again adding validation data...')\n",
    "train_epochs = 30\n",
    "batchSize = 200\n",
    "test_filename = './test_phase_log.csv'\n",
    "csv_logger_2 = CSVLogger(test_filename, separator=',', append=False)\n",
    "my_callback = My_History()\n",
    "checkpoint_2 = ModelCheckpoint('best_taskB2_all_Subjects_CNN_LSTM.h5', monitor='val_acc', verbose=1, save_best_only=True)\n",
    "reduce_lr_2 = ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=5, mode='max', verbose=1, min_lr=0)\n",
    "all_train = np.concatenate((reshaped_train, reshaped_val))\n",
    "all_labels = np.concatenate((train_labels, val_labels))\n",
    "model.fit(all_train,all_labels,validation_data=(reshaped_test,test_labels),epochs=train_epochs,batch_size=batchSize,callbacks=[reduce_lr_2, csv_logger_2, my_callback, checkpoint_2],verbose=1)\n",
    "\n",
    "print('After other {0} epochs BEST test accuracy is: {1}, and BEST f1-score is {2}'.format(train_epochs, np.amax(my_callback.test_acc), np.amax(my_callback.f1_scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to: results_taskB2_all_Subjects_CNN_LSTM.pkl\n"
     ]
    }
   ],
   "source": [
    "# saving variables\n",
    "open_file = 'results_taskB2_all_Subjects_CNN_LSTM.pkl'\n",
    "print('Saving results to: ' + open_file)\n",
    "with open(open_file, 'wb') as f:\n",
    "    pk.dump([my_callback.test_acc, my_callback.f1_scores, my_callback.f1_scores_avg, my_callback.f1_scores_epoch], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = load_model('best_taskB2_all_Subjects_CNN_LSTM.h5')\n",
    "predictions = best_model.predict(reshaped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy is 0.9162118455629674\n",
      "0 0.832524762482 0.958662832495\n",
      "1 0.0058621386699 0.704918032787\n",
      "2 0.00960177885587 0.896174863388\n",
      "3 0.00606428138266 0.730769230769\n",
      "4 0.00838892257934 0.898203592814\n",
      "5 0.0230442692541 0.703448275862\n",
      "6 0.0161714170204 0.801324503311\n",
      "7 0.0101071356378 0.532710280374\n",
      "8 0.00778249444108 0.554913294798\n",
      "9 0.00394178289873 0.506024096386\n",
      "10 0.00424499696786 0.463768115942\n",
      "11 0.0040428542551 0.514285714286\n",
      "12 0.00262785526582 0.464285714286\n",
      "13 0.0067717808773 0.69918699187\n",
      "14 0.00616535273903 0.655172413793\n",
      "15 0.0100060642814 0.634146341463\n",
      "16 0.0320396199717 0.721723518851\n",
      "17 0.0106124924196 0.569620253165\n",
      "The weigths sum is 1.0\n",
      "The computed f1-score is 0.9132369328608785\n",
      "The f1-score with sklearn function is 0.9132369328608785\n",
      "Average f1-score is 0.6671854481466055\n"
     ]
    }
   ],
   "source": [
    "# F1-score measure\n",
    "from sklearn.metrics import f1_score\n",
    "num_classes = 18\n",
    "class_predictions = []\n",
    "class_true = []\n",
    "tot_labels = 0.0\n",
    "count = 0.0\n",
    "for pair in zip(predictions, test_labels):\n",
    "    class_predictions.append(np.argmax(pair[0]))\n",
    "    class_true.append(np.argmax(pair[1]))\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    tot_labels += 1.0\n",
    "    \n",
    "print('Standard accuracy is ' + str(count/tot_labels))    \n",
    "\n",
    "unique, counts = np.unique(class_true, return_counts=True)\n",
    "counted_labels = dict(zip(unique, counts))\n",
    "f1_scores = f1_score(class_predictions, class_true, average=None)\n",
    "\n",
    "tot_f1_score = 0.0\n",
    "weights_sum = 0.0\n",
    "for i in range(num_classes):\n",
    "    labels_class_i = counted_labels[i]\n",
    "    weight_i = labels_class_i / tot_labels\n",
    "    weights_sum += weight_i\n",
    "    tot_f1_score += f1_scores[i]*weight_i\n",
    "    print(str(i) + ' ' + str(weight_i) + ' ' + str(f1_scores[i]))\n",
    "\n",
    "    \n",
    "print('The weigths sum is ' + str(weights_sum))\n",
    "print('The computed f1-score is {}'.format(tot_f1_score))\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(class_true, class_predictions, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "layer_name = 'dense_layer'\n",
    "inter_layer_model = Model(inputs=model.input, outputs=model.get_layer(layer_name).output)\n",
    "\n",
    "network_output = inter_layer_model.predict(all_train)\n",
    "\n",
    "# PCA with certain number of principal components\n",
    "pca = PCA(n_components=40)\n",
    "pca.fit(network_output)\n",
    "new_output = pca.transform(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "svm_labels = np.argmax(all_labels, axis=1)\n",
    "\n",
    "# train a one-vs-one multi-class support vector machine\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(new_output, svm_labels)\n",
    "\n",
    "# predict test data\n",
    "svm_pred = clf.predict(pca.transform(inter_layer_model.predict(reshaped_test)))\n",
    "\n",
    "# measure accuracy and f1-score\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "new_test_labels = np.argmax(test_labels, axis=1)\n",
    "for pair in zip(svm_pred, new_test_labels):\n",
    "    if pair[0] == pair[1]:\n",
    "        num += 1.0\n",
    "    \n",
    "    den += 1.0\n",
    "\n",
    "scores = cross_val_score(clf, pca.transform(inter_layer_model.predict(reshaped_test)),new_test_labels, cv=5)\n",
    "print(scores)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "print('Test accuracy is: {}'.format(num / den))\n",
    "f1_scores = f1_score(svm_pred, new_test_labels, average=None)\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(new_test_labels, svm_pred, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "predicted = cross_val_predict(clf, pca.transform(inter_layer_model.predict(reshaped_test)),new_test_labels, cv=5)\n",
    "\n",
    "# measure accuracy and f1-score\n",
    "num = 0.0\n",
    "den = 0.0\n",
    "new_test_labels = np.argmax(test_labels, axis=1)\n",
    "for pair in zip(predicted, new_test_labels):\n",
    "    if pair[0] == pair[1]:\n",
    "        num += 1.0\n",
    "    \n",
    "    den += 1.0\n",
    "\n",
    "print('Test accuracy is: {}'.format(num / den))\n",
    "f1_scores = f1_score(predicted, new_test_labels, average=None)\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(new_test_labels, predicted, average='weighted')))\n",
    "print('Average f1-score is {}'.format(np.mean(f1_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.to_csv('preds_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(testY)\n",
    "true_df.to_csv('true_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
