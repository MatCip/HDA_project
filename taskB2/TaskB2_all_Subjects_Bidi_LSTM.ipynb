{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, LSTM, CuDNNLSTM, Flatten, Dropout, Reshape, Bidirectional\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slidingWindow(sequence, labels, winSize, step, noNull):\n",
    "\n",
    "    # Verify the inputs\n",
    "    try: it = iter(sequence)\n",
    "    except TypeError:\n",
    "        raise Exception(\"**ERROR** sequence must be iterable.\")\n",
    "    if not ((type(winSize) == type(0)) and (type(step) == type(0))):\n",
    "        raise Exception(\"**ERROR** type(winSize) and type(step) must be int.\")\n",
    "    if step > winSize:\n",
    "        raise Exception(\"**ERROR** step must not be larger than winSize.\")\n",
    "    if winSize > len(sequence):\n",
    "        raise Exception(\"**ERROR** winSize must not be larger than sequence length.\")\n",
    " \n",
    "    # number of chunks\n",
    "    numOfChunks = ((len(sequence)-winSize)//step)+1\n",
    " \n",
    "    # Do the work\n",
    "    for i in range(0,numOfChunks*step,step):\n",
    "        segment = sequence[i:i+winSize]\n",
    "        seg_labels = labels[i:i+winSize]\n",
    "        if noNull:\n",
    "            if seg_labels[-1] != 0:\n",
    "                yield segment, seg_labels\n",
    "        else:\n",
    "            yield segment, seg_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segment_data(X_train, y_train, X_val, y_val, X_test, y_test, winSize, step, noNull=False):\n",
    "    assert len(X_train) == len(y_train)\n",
    "    assert len(X_val) == len(y_val)\n",
    "    assert len(X_test) == len(y_test)\n",
    "    # obtain chunks of data\n",
    "    train_chunks = slidingWindow(X_train, y_train , winSize, step, noNull)\n",
    "    val_chunks = slidingWindow(X_val, y_val, winSize, step, noNull)\n",
    "    test_chunks = slidingWindow(X_test, y_test, winSize, step, noNull)\n",
    "    \n",
    "    # segment the data\n",
    "    train_segments = []\n",
    "    train_labels = []\n",
    "    for chunk in train_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        train_segments.append(data)\n",
    "        train_labels.append(labels[-1])\n",
    "        \n",
    "    val_segments = []\n",
    "    val_labels = []\n",
    "    for chunk in val_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        val_segments.append(data)\n",
    "        val_labels.append(labels[-1])\n",
    "    \n",
    "    test_segments = []\n",
    "    test_labels = []\n",
    "    for chunk in test_chunks:\n",
    "        data = chunk[0]\n",
    "        labels = chunk[1]\n",
    "        test_segments.append(data)\n",
    "        test_labels.append(labels[-1])\n",
    "        \n",
    "    return np.array(train_segments), np.array(train_labels), np.array(val_segments), np.array(val_labels), np.array(test_segments), np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(train_data, val_data, test_data):\n",
    "    encoder = OneHotEncoder()\n",
    "    train_labels = encoder.fit_transform(train_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    val_labels = encoder.transform(val_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    test_labels = encoder.transform(test_data['labels'].values.reshape(-1,1)).toarray()\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_data.drop(['labels'], axis=1, inplace=True)\n",
    "    val_data.drop(['labels'], axis=1, inplace=True)\n",
    "    test_data.drop(['labels'], axis=1, inplace=True)\n",
    "    data = pd.concat([train_data,val_data,test_data])\n",
    "    scaler.fit(data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    val_data = scaler.transform(val_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import train data\n",
    "adl_1_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL1Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL2Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL3Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_4 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL4Opportunity_taskB2_S1.csv\",header=None)\n",
    "adl_1_5 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL5Opportunity_taskB2_S1.csv\",header=None)\n",
    "drill_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/Drill1Opportunity_taskB2.csv\",header=None)\n",
    "adl_2_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL1Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL2Opportunity_taskB2_S2.csv\",header=None)\n",
    "drill_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/Drill2Opportunity_taskB2.csv\",header=None)\n",
    "adl_3_1 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL1Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_2 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL2Opportunity_taskB2_S3.csv\",header=None)\n",
    "drill_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/Drill3Opportunity_taskB2.csv\",header=None)\n",
    "\n",
    "# import validation data\n",
    "adl_2_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL3Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_3 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL3Opportunity_taskB2_S3.csv\",header=None)\n",
    "\n",
    "# import test data\n",
    "adl_2_4 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL4Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_2_5 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL5Opportunity_taskB2_S2.csv\",header=None)\n",
    "adl_3_4 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL4Opportunity_taskB2_S3.csv\",header=None)\n",
    "adl_3_5 = pd.read_csv(\"./full_dataset/CIP_interpolation/ADL5Opportunity_taskB2_S3.csv\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frames = [adl_1_1,adl_1_2,adl_1_3,adl_1_4,adl_1_5,drill_1,adl_2_1,adl_2_2,drill_2,adl_3_1,adl_3_2,drill_3]\n",
    "val_frames = [adl_2_3,adl_3_3]\n",
    "test_frames = [adl_2_4,adl_2_5,adl_3_4,adl_3_5]\n",
    "train_data = pd.concat(train_frames)\n",
    "val_data = pd.concat(val_frames)\n",
    "test_data = pd.concat(test_frames)\n",
    "train_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "val_data.rename(columns ={113: 'labels'}, inplace =True)\n",
    "test_data.rename(columns ={113: 'labels'}, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes: train (497014, 114), val (60949, 114), test (118750, 114)\n"
     ]
    }
   ],
   "source": [
    "print(\"shapes: train {0}, val {1}, test {2}\".format(train_data.shape, val_data.shape, test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_train, scaled_val, scaled_test, train_labels, val_labels, test_labels = prepare_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497014, 113)\n",
      "(497014, 18)\n"
     ]
    }
   ],
   "source": [
    "print(scaled_train.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_sensors = 113\n",
    "window_size = 24\n",
    "step_size = 24\n",
    "classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_segments, train_labels, val_segments, val_labels, test_segments, test_labels = segment_data(scaled_train, train_labels, scaled_val, val_labels,\n",
    "                                                                                                  scaled_test, test_labels, window_size, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "num_cells = 24\n",
    "dropout_prob = 0.5\n",
    "inputshape = (window_size, num_sensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Bidirectional(LSTM(num_cells,kernel_initializer='glorot_normal',return_sequences=True),\n",
    "                        input_shape=inputshape))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(num_cells,kernel_initializer='glorot_normal',return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(num_cells,kernel_initializer='glorot_normal'),merge_mode='ave'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes,kernel_initializer='glorot_normal',\n",
    "                bias_initializer=initializers.Constant(value=0.1),activation='softmax', name='softmax_layer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional_21: input shape: (None, 24, 113) output shape: (None, 24, 48)\n",
      "dropout_1: input shape: (None, 24, 48) output shape: (None, 24, 48)\n",
      "bidirectional_22: input shape: (None, 24, 48) output shape: (None, 24, 48)\n",
      "dropout_2: input shape: (None, 24, 48) output shape: (None, 24, 48)\n",
      "bidirectional_23: input shape: (None, 24, 48) output shape: (None, 24)\n",
      "dropout_3: input shape: (None, 24) output shape: (None, 24)\n",
      "softmax_layer: input shape: (None, 24) output shape: (None, 18)\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(str(layer.name) + ': input shape: ' + str(layer.input_shape) + ' output shape: ' + str(layer.output_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_21 (Bidirectio (None, 24, 48)            26496     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 48)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 24, 48)            14016     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 48)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_23 (Bidirectio (None, 24)                14016     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "softmax_layer (Dense)        (None, 18)                450       \n",
      "=================================================================\n",
      "Total params: 54,978\n",
      "Trainable params: 54,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20708 samples, validate on 2539 samples\n",
      "Epoch 1/100\n",
      "20708/20708 [==============================] - 47s 2ms/step - loss: 1.8338 - acc: 0.6501 - val_loss: 1.0244 - val_acc: 0.8169\n",
      "Epoch 2/100\n",
      "20708/20708 [==============================] - 37s 2ms/step - loss: 1.5836 - acc: 0.6798 - val_loss: 1.0133 - val_acc: 0.8169\n",
      "Epoch 3/100\n",
      "20708/20708 [==============================] - 37s 2ms/step - loss: 1.5430 - acc: 0.6799 - val_loss: 0.9585 - val_acc: 0.8169\n",
      "Epoch 4/100\n",
      "20708/20708 [==============================] - 37s 2ms/step - loss: 1.4682 - acc: 0.6797 - val_loss: 0.8900 - val_acc: 0.8169\n",
      "Epoch 5/100\n",
      "20708/20708 [==============================] - 37s 2ms/step - loss: 1.3977 - acc: 0.6794 - val_loss: 0.8639 - val_acc: 0.8169\n",
      "Epoch 6/100\n",
      "19600/20708 [===========================>..] - ETA: 1s - loss: 1.3568 - acc: 0.6783"
     ]
    }
   ],
   "source": [
    "batchSize = 200\n",
    "train_epoches = 100\n",
    "\n",
    "model.fit(train_segments,train_labels,validation_data=(val_segments,val_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)\n",
    "\n",
    "print('Calculating score.. ')\n",
    "score = model.evaluate(test_segments,test_labels,verbose=1)\n",
    "print(score)\n",
    "model.save('taskB2_all_Subjects_BIDI_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46494 samples, validate on 9894 samples\n",
      "Epoch 1/20\n",
      "46494/46494 [==============================] - 57s 1ms/step - loss: 0.1188 - acc: 0.9725 - val_loss: 0.4993 - val_acc: 0.8899\n",
      "Epoch 2/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0981 - acc: 0.9741 - val_loss: 0.4215 - val_acc: 0.9061\n",
      "Epoch 3/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0894 - acc: 0.9763 - val_loss: 0.4779 - val_acc: 0.9002\n",
      "Epoch 4/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0814 - acc: 0.9776 - val_loss: 0.4861 - val_acc: 0.9042\n",
      "Epoch 5/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0779 - acc: 0.9786 - val_loss: 0.5358 - val_acc: 0.9038\n",
      "Epoch 6/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0737 - acc: 0.9792 - val_loss: 0.4544 - val_acc: 0.9066\n",
      "Epoch 7/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0673 - acc: 0.9816 - val_loss: 0.5327 - val_acc: 0.8968\n",
      "Epoch 8/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0649 - acc: 0.9819 - val_loss: 0.5424 - val_acc: 0.9090\n",
      "Epoch 9/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0590 - acc: 0.9833 - val_loss: 0.5306 - val_acc: 0.9039\n",
      "Epoch 10/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0631 - acc: 0.9830 - val_loss: 0.5455 - val_acc: 0.9054\n",
      "Epoch 11/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0574 - acc: 0.9834 - val_loss: 0.5539 - val_acc: 0.9011\n",
      "Epoch 12/20\n",
      "46494/46494 [==============================] - 57s 1ms/step - loss: 0.0575 - acc: 0.9837 - val_loss: 0.5814 - val_acc: 0.9056\n",
      "Epoch 13/20\n",
      "46494/46494 [==============================] - 57s 1ms/step - loss: 0.0538 - acc: 0.9844 - val_loss: 0.6105 - val_acc: 0.9046\n",
      "Epoch 14/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0530 - acc: 0.9857 - val_loss: 0.5656 - val_acc: 0.9039\n",
      "Epoch 15/20\n",
      "46494/46494 [==============================] - 57s 1ms/step - loss: 0.0508 - acc: 0.9865 - val_loss: 0.6282 - val_acc: 0.9024\n",
      "Epoch 16/20\n",
      "46494/46494 [==============================] - 55s 1ms/step - loss: 0.0520 - acc: 0.9855 - val_loss: 0.5994 - val_acc: 0.9034\n",
      "Epoch 17/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0498 - acc: 0.9865 - val_loss: 0.6566 - val_acc: 0.9019\n",
      "Epoch 18/20\n",
      "46494/46494 [==============================] - 56s 1ms/step - loss: 0.0465 - acc: 0.9876 - val_loss: 0.6197 - val_acc: 0.8985\n",
      "Epoch 19/20\n",
      "46494/46494 [==============================] - 55s 1ms/step - loss: 0.0558 - acc: 0.9868 - val_loss: 0.6682 - val_acc: 0.9060\n",
      "Epoch 20/20\n",
      "46494/46494 [==============================] - 55s 1ms/step - loss: 0.0445 - acc: 0.9880 - val_loss: 0.6687 - val_acc: 0.8979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c9ed40f60>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_epoches = 20\n",
    "all_train = np.concatenate((reshaped_train, reshaped_val))\n",
    "all_labels = np.concatenate((train_labels, val_labels))\n",
    "model.fit(all_train,all_labels,validation_data=(reshaped_test,test_labels),epochs=train_epoches,batch_size=batchSize,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(reshaped_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard accuracy is 0.8979179300586214\n",
      "0 0.832524762482 0.946513446844\n",
      "1 0.0058621386699 0.571428571429\n",
      "2 0.00960177885587 0.887640449438\n",
      "3 0.00606428138266 0.625\n",
      "4 0.00838892257934 0.88\n",
      "5 0.0230442692541 0.661654135338\n",
      "6 0.0161714170204 0.747404844291\n",
      "7 0.0101071356378 0.616113744076\n",
      "8 0.00778249444108 0.51497005988\n",
      "9 0.00394178289873 0.567567567568\n",
      "10 0.00424499696786 0.394736842105\n",
      "11 0.0040428542551 0.523076923077\n",
      "12 0.00262785526582 0.52\n",
      "13 0.0067717808773 0.651162790698\n",
      "14 0.00616535273903 0.64406779661\n",
      "15 0.0100060642814 0.533333333333\n",
      "16 0.0320396199717 0.615384615385\n",
      "17 0.0106124924196 0.573170731707\n",
      "The weigths sum is 1.0\n",
      "The computed f1-score is 0.8955204164504978\n",
      "The f1-score with sklearn function is 0.8955204164504977\n"
     ]
    }
   ],
   "source": [
    "# F1-score measure\n",
    "from sklearn.metrics import f1_score\n",
    "num_classes = 18\n",
    "class_predictions = []\n",
    "class_true = []\n",
    "tot_labels = 0.0\n",
    "count = 0.0\n",
    "for pair in zip(predictions, test_labels):\n",
    "    class_predictions.append(np.argmax(pair[0]))\n",
    "    class_true.append(np.argmax(pair[1]))\n",
    "    if np.argmax(pair[0]) == np.argmax(pair[1]):\n",
    "        count += 1.0\n",
    "    tot_labels += 1.0\n",
    "    \n",
    "print('Standard accuracy is ' + str(count/tot_labels))    \n",
    "\n",
    "unique, counts = np.unique(class_true, return_counts=True)\n",
    "counted_labels = dict(zip(unique, counts))\n",
    "f1_scores = f1_score(class_predictions, class_true, average=None)\n",
    "\n",
    "tot_f1_score = 0.0\n",
    "weights_sum = 0.0\n",
    "for i in range(num_classes):\n",
    "    labels_class_i = counted_labels[i]\n",
    "    weight_i = labels_class_i / tot_labels\n",
    "    weights_sum += weight_i\n",
    "    tot_f1_score += f1_scores[i]*weight_i\n",
    "    print(str(i) + ' ' + str(weight_i) + ' ' + str(f1_scores[i]))\n",
    "\n",
    "    \n",
    "print('The weigths sum is ' + str(weights_sum))\n",
    "print('The computed f1-score is {}'.format(tot_f1_score))\n",
    "print('The f1-score with sklearn function is {}'.format(f1_score(class_true, class_predictions, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df.to_csv('preds_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(testY)\n",
    "true_df.to_csv('true_test.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
